"""\n+Phase 1 Step3: Base GP (Matern 2.5 + ARD) æ„å»ºä¸è®¾è®¡ç©ºé—´æ‰«æ\n+\n+åŠŸèƒ½æ¦‚è¿°:\n+1. è¯»å– Phase1 æ•°æ®é›† (å«å› å­ä¸å“åº”) \n+2. å¯¹æ¯ä¸ªè¢«è¯•è¿›è¡Œè¢«è¯•å†… Z-score æ ‡å‡†åŒ– (y -> y_norm)\n+3. ä½¿ç”¨ Matern Î½=2.5 Kernel + ARD è®­ç»ƒ Base GP (botorch + gpytorch)\n+4. æ‰«æç”¨æˆ·ç»™å®šçš„è®¾è®¡ç©ºé—´ CSV, è®¡ç®—é¢„æµ‹å‡å€¼/æ ‡å‡†å·®\n+5. é€‰å‡º: å…¨å±€æœ€é«˜ç‚¹ x_best_prior, å…¨å±€æœ€ä½ç‚¹ x_worst_prior, æœ€ä¸ç¡®å®šç‚¹ x_max_std (è‹¥æ–¹å·®è¿‡ä½åˆ™é€€åŒ–ä¸ºâ€œä¸­å¿ƒç‚¹â€)\n+6. å¯¼å‡ºæ¨¡å‹ state_dict, é•¿åº¦å°ºåº¦, å…³é”®ç‚¹, è®¾è®¡ç©ºé—´æ‰«æç»“æœ, æŠ¥å‘Š\n+\n+ä½¿ç”¨æ–¹å¼(äº¤äº’):\n+  python phase1_step3_base_gp.py\n+  -> è¾“å…¥ Phase1 æ•°æ® CSV è·¯å¾„ / è®¾è®¡ç©ºé—´ CSV è·¯å¾„ ç­‰\n+\n+ä½¿ç”¨æ–¹å¼(é…ç½®ä¸€æ¬¡æ€§è°ƒç”¨, æ¨èç»“åˆ quick_start.py):\n+  åœ¨ quick_start.py ä¸­è®¾ç½® MODE='step3' å¹¶å¡«å†™ STEP3_CONFIG\n+\n+æ–‡ä»¶è¾“å‡º(é»˜è®¤ output_dir=base_gp_output):\n+  base_gp_state.pth               æ¨¡å‹ä¸likelihood state_dict
  base_gp_lengthscales.json       é•¿åº¦å°ºåº¦ä¸æ•æ„Ÿåº¦æ’åº
  base_gp_subject_stats.json      è¢«è¯•æ ‡å‡†åŒ–ç»Ÿè®¡ (å‡å€¼/æ ‡å‡†å·®)
  base_gp_encodings.json          åˆ†ç±»å˜é‡ç¼–ç æ˜ å°„
  base_gp_key_points.json         ä¸‰ä¸ªå…³é”®ç‚¹åŠé¢„æµ‹å€¼
  design_space_scan.csv           è®¾è®¡ç©ºé—´é€ç‚¹é¢„æµ‹ (mean,std)
  base_gp_report.md               æŠ¥å‘Šæ‘˜è¦
\n+ä¾èµ–: éœ€è¦å·²å®‰è£… torch, gpytorch, botorch (åœ¨å½“å‰ pixi ç¯å¢ƒä¸­ aepsych å·²ä¾èµ– botorch)ã€‚\n+"""

from __future__ import annotations

import json
import math
import sys
from pathlib import Path
from typing import Dict, Any, Tuple, List

import numpy as np
import pandas as pd

try:
    import torch
    from torch import Tensor
    import gpytorch
    from botorch.models import SingleTaskGP
    from botorch.optim.fit import fit_gpytorch_mll_torch
    from gpytorch.mlls import ExactMarginalLogLikelihood
except Exception as e:  # pragma: no cover - ç¯å¢ƒå¯¼å…¥å¤±è´¥æ—¶çš„æç¤º
    print("[é”™è¯¯] éœ€è¦å®‰è£… torch/gpytorch/botorch: ", e)
    sys.exit(1)


def _infer_encoding_from_sampling(
    data_dir: Path, factor_cols: List[str]
) -> Dict[str, Dict[Any, int]]:
    """ä»é‡‡æ ·æ–¹æ¡ˆå’Œæ¨¡æ‹Ÿç»“æœæ¨æ–­ç¼–ç æ˜ å°„ã€‚

    æ¯”è¾ƒ subject_1.csv (categorical) å’Œ result/subject_1.csv (numeric)
    æ¥æ¨æ–­å“ªäº›åˆ—è¢«ç¼–ç äº†ï¼Œä»¥åŠç¼–ç æ˜ å°„æ˜¯ä»€ä¹ˆã€‚
    """
    encodings: Dict[str, Dict[Any, int]] = {}

    # æŸ¥æ‰¾é‡‡æ ·æ–¹æ¡ˆæ–‡ä»¶å’Œç»“æœæ–‡ä»¶
    sampling_file = data_dir.parent / "subject_1.csv"
    result_file = data_dir / "subject_1.csv"

    if not sampling_file.exists() or not result_file.exists():
        print(f"[Warning] æ— æ³•æ¨æ–­ç¼–ç ï¼šæ‰¾ä¸åˆ°é‡‡æ ·æ–‡ä»¶æˆ–ç»“æœæ–‡ä»¶")
        return encodings

    df_sampling = pd.read_csv(sampling_file)
    df_result = pd.read_csv(result_file)

    # å¯¹äºæ¯ä¸ªå› å­åˆ—ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦ç¼–ç 
    for col in factor_cols:
        if col not in df_sampling.columns or col not in df_result.columns:
            continue

        # å¦‚æœé‡‡æ ·æ˜¯categoricalï¼Œç»“æœæ˜¯numericï¼Œåˆ™æ¨æ–­ç¼–ç 
        if df_sampling[col].dtype == "object" and df_result[col].dtype != "object":
            # æ”¶é›†æ‰€æœ‰ (categorical_value, numeric_value) å¯¹
            mapping_pairs = []
            for i in range(min(len(df_sampling), len(df_result))):
                cat_val = df_sampling[col].iloc[i]
                num_val = df_result[col].iloc[i]
                if pd.notna(cat_val) and pd.notna(num_val):
                    mapping_pairs.append((cat_val, int(num_val)))

            # æ„å»ºæ˜ å°„å­—å…¸
            mapping = {}
            for cat_val, num_val in mapping_pairs:
                if cat_val not in mapping:
                    mapping[cat_val] = num_val
                elif mapping[cat_val] != num_val:
                    print(f"[Warning] åˆ— {col} çš„ç¼–ç ä¸ä¸€è‡´: {cat_val} -> {mapping[cat_val]} vs {num_val}")

            if mapping:
                encodings[col] = mapping
                print(f"[æ¨æ–­ç¼–ç ] {col}: {mapping}")

    return encodings


def _encode_factor_df(
    df: pd.DataFrame,
) -> Tuple[pd.DataFrame, Dict[str, Dict[Any, int]]]:
    """å¯¹å› å­åˆ—è¿›è¡Œç¼–ç  (åˆ†ç±»å˜é‡ label encode, bool->int)ã€‚
    è¿”å›ç¼–ç å DataFrame ä¸ ç¼–ç å­—å…¸ã€‚"""
    encoded = df.copy()
    encodings: Dict[str, Dict[Any, int]] = {}
    for col in encoded.columns:
        if encoded[col].dtype == "object":
            unique_vals = sorted(encoded[col].dropna().unique())
            mapping = {v: i for i, v in enumerate(unique_vals)}
            encodings[col] = mapping
            encoded[col] = encoded[col].map(mapping)
        elif encoded[col].dtype == "bool":
            mapping = {False: 0, True: 1}
            encodings[col] = mapping
            encoded[col] = encoded[col].astype(int)
    return encoded, encodings


def _apply_encodings(
    df: pd.DataFrame, encodings: Dict[str, Dict[Any, int]]
) -> pd.DataFrame:
    """å°†å·²å­˜åœ¨çš„ç¼–ç æ˜ å°„åº”ç”¨åˆ°æ–°çš„ DataFrame (è®¾è®¡ç©ºé—´). æ–°å‡ºç°çš„ç±»åˆ«æŠ¥é”™ã€‚"""
    df_new = df.copy()

    # å¤„ç†æ‰€æœ‰åˆ—ï¼Œç¡®ä¿æ²¡æœ‰é—æ¼çš„åˆ†ç±»å˜é‡
    for col in df_new.columns:
        if col in encodings:
            # åˆ—åœ¨ç¼–ç å­—å…¸ä¸­ - åº”ç”¨ç¼–ç 
            mapping = encodings[col]
            if df_new[col].dtype == "object":
                unknown = set(df_new[col].dropna().unique()) - set(mapping.keys())
                if unknown:
                    raise ValueError(f"è®¾è®¡ç©ºé—´åˆ— {col} å‡ºç°æœªçŸ¥ç±»åˆ«: {unknown}")
                df_new[col] = df_new[col].map(mapping)
            elif df_new[col].dtype == "bool":
                df_new[col] = df_new[col].astype(int)
        else:
            # åˆ—ä¸åœ¨ç¼–ç å­—å…¸ä¸­ - æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†ç±»å˜é‡ï¼ˆè¿™æ˜¯é”™è¯¯ï¼‰
            if df_new[col].dtype == "object":
                raise ValueError(
                    f"è®¾è®¡ç©ºé—´åˆ— '{col}' æ˜¯åˆ†ç±»å˜é‡ï¼Œä½†è®­ç»ƒæ•°æ®ä¸­è¯¥åˆ—ä¸ºæ•°å€¼å‹ã€‚"
                    f"è¯·ç¡®ä¿è®­ç»ƒæ•°æ®å’Œè®¾è®¡ç©ºé—´çš„åˆ—ç±»å‹ä¸€è‡´ã€‚"
                    f"å½“å‰å€¼ç¤ºä¾‹: {df_new[col].head().tolist()}"
                )
            elif df_new[col].dtype == "bool":
                # å¸ƒå°”å‹ä¹Ÿéœ€è¦è½¬æ¢
                df_new[col] = df_new[col].astype(int)

    return df_new


def _standardize_subject_wise(
    df: pd.DataFrame, subject_col: str, response_col: str
) -> Tuple[np.ndarray, Dict[str, Dict[str, float]]]:
    """è¢«è¯•å†… Z-score æ ‡å‡†åŒ–ã€‚
    ä»…è¿”å›æ ‡å‡†åŒ–åçš„ y_norm ä»¥åŠ subject_statsï¼›ä¸å¯¹ X åšä»»ä½•å¤„ç†ï¼ˆç”±å¤–éƒ¨ç¼–ç ï¼‰ã€‚"""
    subject_stats: Dict[str, Dict[str, float]] = {}
    y = df[response_col].values.astype(float)
    subjects = df[subject_col].astype(str).values
    y_norm = np.zeros_like(y)
    global_std = float(np.std(y)) + 1e-6
    for subj in np.unique(subjects):
        mask = subjects == subj
        y_subj = y[mask]
        mean_subj = float(np.mean(y_subj))
        std_subj = float(np.std(y_subj))
        adj_std = std_subj if std_subj > 1e-8 else global_std
        y_norm[mask] = (y_subj - mean_subj) / (adj_std + 1e-12)
        subject_stats[subj] = {
            "mean": mean_subj,
            "std": std_subj,
            "adjusted_std_used": adj_std,
            "n": int(mask.sum()),
        }
    return y_norm.astype(float), subject_stats


class _MaternARDGP(gpytorch.models.ExactGP):
    """è‡ªå®šä¹‰ Matern 2.5 + ARD ç²¾ç¡® GP."""

    def __init__(
        self,
        train_x: Tensor,
        train_y: Tensor,
        likelihood: gpytorch.likelihoods.GaussianLikelihood,
    ):
        super().__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.MaternKernel(
                nu=2.5,
                ard_num_dims=train_x.shape[-1],
            )
        )

    def forward(self, x: Tensor):  # type: ignore
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)


def train_base_gp(
    train_x: np.ndarray,
    train_y: np.ndarray,
    max_iters: int = 300,
    lr: float = 0.05,
    use_cuda: bool = True,
) -> Tuple[_MaternARDGP, gpytorch.likelihoods.GaussianLikelihood, Dict[str, Any]]:
    """è®­ç»ƒè‡ªå®šä¹‰ Matern2.5+ARD GPã€‚è¿”å›æ¨¡å‹, likelihood, è®­ç»ƒæ—¥å¿—ã€‚"""
    device = torch.device("cuda" if use_cuda and torch.cuda.is_available() else "cpu")
    X = torch.from_numpy(train_x).float().to(device)
    y = torch.from_numpy(train_y).float().to(device)
    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)
    model = _MaternARDGP(X, y, likelihood).to(device)
    model.train()
    likelihood.train()
    optimizer = torch.optim.Adam([{"params": model.parameters()}], lr=lr)
    mll = ExactMarginalLogLikelihood(likelihood, model)
    log_history: List[Dict[str, float]] = []

    for it in range(1, max_iters + 1):
        optimizer.zero_grad()
        output = model(X)
        loss = -mll(output, y)
        loss.backward()
        optimizer.step()
        if it % 25 == 0 or it == 1:
            lengthscales = (
                model.covar_module.base_kernel.lengthscale.detach()
                .cpu()
                .numpy()
                .ravel()
                .tolist()
            )
            log_history.append(
                {
                    "iter": it,
                    "loss": float(loss.item()),
                    "noise": float(model.likelihood.noise.item()),
                    "lengthscale_mean": float(np.mean(lengthscales)),
                }
            )
    model.eval()
    likelihood.eval()
    return model, likelihood, {"device": str(device), "history": log_history}


def scan_design_space(
    model: _MaternARDGP,
    likelihood: gpytorch.likelihoods.GaussianLikelihood,
    design_x: np.ndarray,
    batch_size: int = 2048,
) -> Tuple[np.ndarray, np.ndarray]:
    """æ‰¹é‡é¢„æµ‹è®¾è®¡ç©ºé—´ (å‡å€¼, æ ‡å‡†å·®)ã€‚"""
    device = next(model.parameters()).device
    means: List[np.ndarray] = []
    stds: List[np.ndarray] = []
    with torch.no_grad(), gpytorch.settings.fast_pred_var():
        for start in range(0, design_x.shape[0], batch_size):
            end = start + batch_size
            Xbatch = torch.from_numpy(design_x[start:end]).float().to(device)
            pred = likelihood(model(Xbatch))
            means.append(pred.mean.cpu().numpy())
            stds.append(pred.stddev.cpu().numpy())
    mean_all = np.concatenate(means, axis=0)
    std_all = np.concatenate(stds, axis=0)
    return mean_all, std_all


def select_key_points(
    design_df_encoded: pd.DataFrame,
    means: np.ndarray,
    stds: np.ndarray,
    ensure_diversity: bool = True,
) -> Dict[str, Any]:
    """é€‰æ‹©ä¸‰ä¸ªå…³é”®ç‚¹ã€‚è‹¥æœ€å¤§ std < 1e-6 åˆ™ç”¨"ä¸­å¿ƒç‚¹"æ›¿ä»£ä¸ç¡®å®šç‚¹ã€‚

    Args:
        ensure_diversity: è‹¥Trueï¼Œè‹¥Sample 3ä¸Sample 1/2é‡å¤ï¼Œåˆ™é€‰Stdç¬¬äºŒé«˜çš„ç‚¹ã€‚
    """
    idx_best = int(np.argmax(means))
    idx_worst = int(np.argmin(means))
    max_std = float(np.max(stds))
    center_point = design_df_encoded.median(numeric_only=True).to_dict()

    # åˆå§‹é€‰æ‹©ï¼šæœ€å¤§stdçš„ç‚¹
    if max_std < 1e-6:
        idx_std = -1  # æ ‡è®°ä½¿ç”¨ä¸­å¿ƒç‚¹
        max_std_mean = None
    else:
        idx_std = int(np.argmax(stds))
        max_std_mean = float(means[idx_std])

        # è‹¥å¯ç”¨å¤šæ ·æ€§æ£€æŸ¥ï¼šç¡®ä¿Sample 3ä¸Sample 1/2ä¸é‡å¤
        if ensure_diversity and idx_std in (idx_best, idx_worst):
            # æ‰¾ç¬¬äºŒé«˜ã€ç¬¬ä¸‰é«˜ç­‰çš„stdç‚¹ï¼ˆä¸”ä¸æ˜¯best/worstï¼‰
            sorted_indices = np.argsort(-stds)  # é™åºæ’åˆ—
            for candidate_idx in sorted_indices:
                if candidate_idx not in (idx_best, idx_worst):
                    idx_std = int(candidate_idx)
                    max_std = float(stds[idx_std])
                    max_std_mean = float(means[idx_std])
                    break

    return {
        "x_best_prior_index": idx_best,
        "x_best_prior": design_df_encoded.iloc[idx_best].to_dict(),
        "best_mean": float(means[idx_best]),
        "best_std": float(stds[idx_best]),
        "x_worst_prior_index": idx_worst,
        "x_worst_prior": design_df_encoded.iloc[idx_worst].to_dict(),
        "worst_mean": float(means[idx_worst]),
        "worst_std": float(stds[idx_worst]),
        "x_max_std_index": idx_std,
        "x_max_std": (
            design_df_encoded.iloc[idx_std].to_dict() if idx_std >= 0 else center_point
        ),
        "max_std": max_std if idx_std >= 0 else max_std,
        "max_std_mean": max_std_mean,
        "used_center_point": idx_std == -1,
        "center_point": center_point,
        "ensure_diversity": ensure_diversity,
    }


def write_report(
    path: Path,
    factor_names: List[str],
    lengthscales: List[float],
    subject_stats: Dict[str, Dict[str, float]],
    key_points: Dict[str, Any],
    train_meta: Dict[str, Any],
):
    """ç”Ÿæˆ Markdown æŠ¥å‘Šã€‚"""
    with open(path, "w", encoding="utf-8") as f:
        f.write("# Base GP (Matern 2.5 + ARD) æŠ¥å‘Š\n\n")
        f.write("## ğŸ“ æ¨¡å‹ç»“æ„\n")
        f.write("- Kernel: Matern(Î½=2.5) + ARD + Scale\n")
        f.write("- è¾“å…¥ç»´åº¦: {}\n".format(len(factor_names)))
        f.write("- è®¾å¤‡: {}\n".format(train_meta.get("device")))
        f.write("\n## ğŸ”§ è®­ç»ƒæ‘˜è¦\n")
        hist = train_meta.get("history", [])
        if hist:
            f.write(
                "| Iter | Loss | Noise | Mean Lengthscale |\n|------|------|-------|------------------|\n"
            )
            for row in hist:
                f.write(
                    f"| {row['iter']} | {row['loss']:.3f} | {row['noise']:.3e} | {row['lengthscale_mean']:.3f} |\n"
                )
        f.write("\n## ğŸ›ï¸ é•¿åº¦å°ºåº¦ (Sensitivity)\n")
        ranked = sorted(zip(factor_names, lengthscales), key=lambda x: x[1])
        f.write(
            "| Rank | Factor | Lengthscale | Interpretation |\n|------|--------|------------:|---------------|\n"
        )
        for rank, (name, ls) in enumerate(ranked, 1):
            interp = (
                "é«˜æ•æ„Ÿ (å˜åŒ–å°å³å½±å“å¤§)"
                if rank <= max(1, len(ranked) // 3)
                else ("ä¸­ç­‰" if rank <= 2 * len(ranked) // 3 else "ä½æ•æ„Ÿ")
            )
            f.write(f"| {rank} | {name} | {ls:.4f} | {interp} |\n")
        f.write("\n## ğŸ‘¥ è¢«è¯•æ ‡å‡†åŒ–ç»Ÿè®¡\n")
        f.write(
            "| Subject | Mean | Std | Adjusted_Std_Used | N |\n|---------|------|-----|-------------------|---|\n"
        )
        for subj, stats in subject_stats.items():
            f.write(
                f"| {subj} | {stats['mean']:.3f} | {stats['std']:.3f} | {stats['adjusted_std_used']:.3f} | {stats['n']} |\n"
            )
        f.write("\n## ğŸ“ å…³é”®ç‚¹ (è®¾è®¡ç©ºé—´) - ä¸‰ä¸ªé‡‡æ ·ç‚¹\n")
        f.write("*ä¾› Phase 2 ç›´æ¥ä½¿ç”¨çš„ä¸‰ä¸ªå…³é”®å‚æ•°é…æ–¹*\n\n")

        # Sample 1: Best Prior
        best_coords = key_points["x_best_prior"]
        best_coord_list = [best_coords[f] for f in factor_names]
        f.write("### 1ï¸âƒ£ Sample 1 (Best Prior)\n")
        f.write(
            "- **Score**: Mean = {:.3f} (Std = {:.3f})\n".format(
                key_points["best_mean"], key_points["best_std"]
            )
        )
        f.write("- **Coordinates**: {}\n".format(best_coord_list))
        f.write("- **Detailed**: ")
        f.write(", ".join([f"{name}={best_coords[name]}" for name in factor_names]))
        f.write("\n\n")

        # Sample 2: Worst Prior
        worst_coords = key_points["x_worst_prior"]
        worst_coord_list = [worst_coords[f] for f in factor_names]
        f.write("### 2ï¸âƒ£ Sample 2 (Worst Prior)\n")
        f.write(
            "- **Score**: Mean = {:.3f} (Std = {:.3f})\n".format(
                key_points["worst_mean"], key_points["worst_std"]
            )
        )
        f.write("- **Coordinates**: {}\n".format(worst_coord_list))
        f.write("- **Detailed**: ")
        f.write(", ".join([f"{name}={worst_coords[name]}" for name in factor_names]))
        f.write("\n\n")

        # Sample 3: Max Uncertainty
        max_std_coords = key_points["x_max_std"]
        max_std_coord_list = [max_std_coords[f] for f in factor_names]
        f.write("### 3ï¸âƒ£ Sample 3 (Max Uncertainty / Center)\n")
        if key_points["used_center_point"]:
            f.write(
                "âš ï¸  **Note**: All points have very low variance (<1e-6), using design space center instead\n\n"
            )
            f.write("- **Score**: Center Point (Std â‰ˆ 0)\n")
        else:
            f.write(
                "- **Score**: Std = {:.3f} (Mean = {:.3f})\n".format(
                    key_points["max_std"], key_points.get("max_std_mean", 0.0)
                )
            )
        f.write("- **Coordinates**: {}\n".format(max_std_coord_list))
        f.write("- **Detailed**: ")
        f.write(", ".join([f"{name}={max_std_coords[name]}" for name in factor_names]))
        f.write("\n\n")
        f.write("\n## ğŸ§ª ä½¿ç”¨ç¤ºä¾‹\n")
        f.write(
            "```python\nimport torch, json, gpytorch\nfrom phase1_step3_base_gp import _MaternARDGP\n# åŠ è½½ state_dict\nstate = torch.load('base_gp_state.pth', map_location='cpu')\n# é‡å»ºæ¨¡å‹ (éœ€çŸ¥é“è¾“å…¥ç»´åº¦)\nD = {d}\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\nmodel = _MaternARDGP(torch.zeros(1, D), torch.zeros(1), likelihood)\nmodel.load_state_dict(state['model'])\nlikelihood.load_state_dict(state['likelihood'])\nmodel.eval(); likelihood.eval()\n# é¢„æµ‹\nwith torch.no_grad():\n    x = torch.randn(5, D)\n    pred = likelihood(model(x))\n    print(pred.mean, pred.stddev)\n```\n".format(
                d=len(factor_names)
            )
        )
        f.write("\n*è‡ªåŠ¨ç”Ÿæˆ*\n")


def run_step3_interactive():  # pragma: no cover - äº¤äº’ä¸»å…¥å£
    print("=" * 80)
    print("Phase 1 Step3: Base GP æ„å»ºä¸æ‰«æ")
    print("=" * 80)
    data_csv = (
        input("Phase1 æ•°æ®CSVè·¯å¾„ (å«å“åº”) [default warmup_data.csv]: ").strip()
        or "warmup_data.csv"
    )
    design_csv = (
        input("è®¾è®¡ç©ºé—´CSVè·¯å¾„ [default design_space.csv]: ").strip()
        or "design_space.csv"
    )
    subject_col = input("è¢«è¯•åˆ—å [default subject_id]: ").strip() or "subject_id"
    response_col = input("å“åº”åˆ—å [default response]: ").strip() or "response"
    output_dir = (
        input("è¾“å‡ºç›®å½• [default base_gp_output]: ").strip() or "base_gp_output"
    )
    max_iters_str = input("è®­ç»ƒè¿­ä»£æ•° [default 300]: ").strip() or "300"
    lr_str = input("å­¦ä¹ ç‡ [default 0.05]: ").strip() or "0.05"
    use_cuda = input("ä½¿ç”¨CUDA? (Y/n): ").strip().lower() != "n"
    try:
        max_iters = int(max_iters_str)
        lr = float(lr_str)
    except ValueError:
        print("[é”™è¯¯] å‚æ•°æ ¼å¼ä¸æ­£ç¡®")
        sys.exit(1)
    process_step3(
        data_csv_path=data_csv,
        design_space_csv=design_csv,
        subject_col=subject_col,
        response_col=response_col,
        output_dir=output_dir,
        max_iters=max_iters,
        lr=lr,
        use_cuda=use_cuda,
    )


def process_step3(
    data_csv_path: str,
    design_space_csv: str,
    subject_col: str,
    response_col: str,
    output_dir: str,
    max_iters: int = 300,
    lr: float = 0.05,
    use_cuda: bool = True,
    ensure_diversity: bool = True,
) -> Dict[str, Any]:
    """æ ¸å¿ƒæµç¨‹ (ä¾› quick_start è°ƒç”¨)ã€‚

    Args:
        data_csv_path: Phase1 æ•°æ®è·¯å¾„
                      - å¦‚æœæ˜¯æ–‡ä»¶: ç›´æ¥è¯»å–ï¼ˆéœ€åŒ…å«subject_colå’Œresponse_colï¼‰
                      - å¦‚æœæ˜¯ç›®å½•: è¯»å–æ‰€æœ‰subject_*.csvï¼Œæ¯ä¸ªæ–‡ä»¶ä»£è¡¨ä¸€ä¸ªè¢«è¯•
        ensure_diversity: è‹¥Trueï¼Œè‹¥Sample 3ä¸Sample 1/2é‡å¤ï¼Œåˆ™é€‰Stdç¬¬äºŒé«˜çš„ç‚¹ã€‚
    """
    data_path = Path(data_csv_path)
    design_path = Path(design_space_csv)
    if not data_path.exists():
        raise FileNotFoundError(f"Phase1 æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {data_csv_path}")
    if not design_path.exists():
        raise FileNotFoundError(f"è®¾è®¡ç©ºé—´æ–‡ä»¶ä¸å­˜åœ¨: {design_space_csv}")

    # æ£€æŸ¥æ˜¯æ–‡ä»¶è¿˜æ˜¯ç›®å½•
    if data_path.is_dir():
        # ç›®å½•æ¨¡å¼ï¼šè¯»å–æ‰€æœ‰ subject_*.csv
        print(f"[Step3] ä»ç›®å½•è¯»å–è¢«è¯•æ•°æ®: {data_csv_path}")
        subject_csvs = sorted(data_path.glob("subject_*.csv"))

        if not subject_csvs:
            raise FileNotFoundError(f"ç›®å½•ä¸­æœªæ‰¾åˆ° subject_*.csv æ–‡ä»¶: {data_csv_path}")

        print(f"  æ‰¾åˆ° {len(subject_csvs)} ä¸ªè¢«è¯•æ–‡ä»¶")

        # è¯»å–æ¯ä¸ªè¢«è¯•æ–‡ä»¶å¹¶æ·»åŠ subjectåˆ—
        all_dfs = []
        for csv_path in subject_csvs:
            df_subject = pd.read_csv(csv_path)

            # éªŒè¯å“åº”åˆ—å­˜åœ¨
            if response_col not in df_subject.columns:
                raise ValueError(f"æ–‡ä»¶ {csv_path.name} ä¸­æœªæ‰¾åˆ°å“åº”åˆ—: '{response_col}'")

            # æ·»åŠ è¢«è¯•åˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if subject_col not in df_subject.columns:
                subject_id = csv_path.stem  # "subject_1"
                df_subject.insert(0, subject_col, subject_id)

            all_dfs.append(df_subject)
            print(f"    - {csv_path.name}: {len(df_subject)} è¡Œ")

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        df_phase1 = pd.concat(all_dfs, ignore_index=True)
        print(f"  åˆå¹¶åæ€»è®¡: {len(df_phase1)} è¡Œ")
    else:
        # æ–‡ä»¶æ¨¡å¼ï¼šç›´æ¥è¯»å–
        print(f"[Step3] è¯»å–æ•°æ®æ–‡ä»¶: {data_csv_path}")
        df_phase1 = pd.read_csv(data_path)

    if subject_col not in df_phase1.columns or response_col not in df_phase1.columns:
        raise ValueError("Phase1 æ•°æ®ç¼ºå°‘å¿…è¦åˆ—")

    factor_cols = [c for c in df_phase1.columns if c not in (subject_col, response_col)]
    factor_df = df_phase1[factor_cols]
    encoded_factors, encodings = _encode_factor_df(factor_df)

    # å¦‚æœæ˜¯ç›®å½•æ¨¡å¼ï¼Œå°è¯•ä»é‡‡æ ·æ–¹æ¡ˆæ¨æ–­é¢å¤–çš„ç¼–ç ï¼ˆç”¨äºè®¾è®¡ç©ºé—´ï¼‰
    if data_path.is_dir():
        print("\n[æ¨æ–­] ä»é‡‡æ ·æ–¹æ¡ˆæ¨æ–­åˆ†ç±»å˜é‡ç¼–ç ...")
        inferred_encodings = _infer_encoding_from_sampling(data_path, factor_cols)
        # åˆå¹¶æ¨æ–­çš„ç¼–ç ï¼ˆä¼˜å…ˆä½¿ç”¨æ¨æ–­çš„ï¼Œå› ä¸ºå®ƒåŒ…å«å®Œæ•´çš„categorical->numericæ˜ å°„ï¼‰
        for col, mapping in inferred_encodings.items():
            if col not in encodings or not encodings[col]:
                encodings[col] = mapping
                print(f"  ä½¿ç”¨æ¨æ–­ç¼–ç : {col}")
            else:
                print(f"  åˆ— {col} å·²æœ‰ç¼–ç ï¼Œè·³è¿‡æ¨æ–­")

    # æ ‡å‡†åŒ– (ä½¿ç”¨åŸå§‹æœªç¼–ç å› å­, ä½†æˆ‘ä»¬åªéœ€è¦ y_norm ä¸ X ç¼–ç åçš„æ•°å€¼)
    X_numeric = encoded_factors
    df_for_std = df_phase1[[subject_col, response_col] + factor_cols]
    y_norm, subject_stats = _standardize_subject_wise(
        df_phase1[[subject_col, response_col]], subject_col, response_col
    )
    X_train = X_numeric.values.astype(float)

    model, likelihood, train_meta = train_base_gp(
        X_train, y_norm, max_iters=max_iters, lr=lr, use_cuda=use_cuda
    )
    lengthscales = (
        model.covar_module.base_kernel.lengthscale.detach()
        .cpu()
        .numpy()
        .ravel()
        .tolist()
    )

    # æ‰«æè®¾è®¡ç©ºé—´
    design_df_raw = pd.read_csv(design_path)
    # åªå–ä¸è®­ç»ƒç›¸åŒçš„å› å­åˆ—, ä¸¢å¼ƒå…¶å®ƒåˆ—
    missing_cols = set(factor_cols) - set(design_df_raw.columns)
    if missing_cols:
        raise ValueError(f"è®¾è®¡ç©ºé—´ç¼ºå°‘å› å­åˆ—: {missing_cols}")
    design_df_aligned = design_df_raw[factor_cols]

    # Debug: æ£€æŸ¥ç¼–ç å‰çš„æ•°æ®ç±»å‹
    print("\n[Debug] è®¾è®¡ç©ºé—´ç¼–ç å‰:")
    for col in design_df_aligned.columns:
        print(f"  {col}: dtype={design_df_aligned[col].dtype}, "
              f"in_encodings={col in encodings}, "
              f"sample_values={design_df_aligned[col].head(3).tolist()}")

    design_df_encoded = _apply_encodings(design_df_aligned, encodings)

    # Debug: æ£€æŸ¥ç¼–ç åçš„æ•°æ®ç±»å‹
    print("\n[Debug] è®¾è®¡ç©ºé—´ç¼–ç å:")
    for col in design_df_encoded.columns:
        print(f"  {col}: dtype={design_df_encoded[col].dtype}, "
              f"sample_values={design_df_encoded[col].head(3).tolist()}")

    means, stds = scan_design_space(
        model, likelihood, design_df_encoded.values.astype(float)
    )
    key_points = select_key_points(
        design_df_encoded, means, stds, ensure_diversity=ensure_diversity
    )

    out_dir = Path(output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    # ä¿å­˜ state_dict
    torch.save(
        {"model": model.state_dict(), "likelihood": likelihood.state_dict()},
        out_dir / "base_gp_state.pth",
    )
    # å…¶å®ƒ JSON
    (out_dir / "base_gp_lengthscales.json").write_text(
        json.dumps(
            {"factor_names": factor_cols, "lengthscales": lengthscales},
            indent=2,
            ensure_ascii=False,
        ),
        encoding="utf-8",
    )
    (out_dir / "base_gp_subject_stats.json").write_text(
        json.dumps(subject_stats, indent=2, ensure_ascii=False), encoding="utf-8"
    )
    (out_dir / "base_gp_encodings.json").write_text(
        json.dumps(encodings, indent=2, ensure_ascii=False), encoding="utf-8"
    )
    (out_dir / "base_gp_key_points.json").write_text(
        json.dumps(key_points, indent=2, ensure_ascii=False), encoding="utf-8"
    )
    # è®¾è®¡ç©ºé—´æ‰«æ CSV
    scan_df = design_df_encoded.copy()
    scan_df["pred_mean"] = means
    scan_df["pred_std"] = stds
    scan_df.to_csv(out_dir / "design_space_scan.csv", index=False)
    # æŠ¥å‘Š
    write_report(
        out_dir / "base_gp_report.md",
        factor_cols,
        lengthscales,
        subject_stats,
        key_points,
        train_meta,
    )

    return {
        "output_dir": str(out_dir),
        "lengthscales": lengthscales,
        "key_points": key_points,
        "n_design_points": int(design_df_encoded.shape[0]),
    }


def main():  # pragma: no cover
    run_step3_interactive()


if __name__ == "__main__":  # pragma: no cover
    main()
