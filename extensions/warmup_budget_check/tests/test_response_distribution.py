#!/usr/bin/env python3
"""
æµ‹è¯•å“åº”åˆ†å¸ƒ - ä½¿ç”¨å®é™…è®¾è®¡ç©ºé—´éªŒè¯æ¨¡æ‹Ÿè¢«è¯•çš„å“åº”åˆ†å¸ƒ

ç›®çš„:
1. ä½¿ç”¨å®é™…è®¾è®¡ç©ºé—´ (i9csy65bljq14ovww2v91-6532622b_JBmIu2QSKA.csv)
2. ç”Ÿæˆå¤šä¸ªæ¨¡æ‹Ÿè¢«è¯•çš„å“åº”
3. åˆ†æå“åº”åˆ†å¸ƒæ˜¯å¦ç¬¦åˆé¢„æœŸ (æ­£æ€æ€§ã€åç§»ç­‰)
4. å¯è§†åŒ–åˆ†å¸ƒå¹¶ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
"""

import sys
from pathlib import Path
import numpy as np
import pandas as pd
import json
from collections import Counter

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parents[3]
sys.path.insert(0, str(project_root / "tools"))

# å¯¼å…¥æ–°çš„v2æ¨¡æ‹Ÿè¢«è¯•æ¨¡å—
try:
    from subject_simulator_v2 import LinearSubject, ClusterGenerator
except ImportError as e:
    print(f"[é”™è¯¯] æ— æ³•å¯¼å…¥ subject_simulator_v2: {e}")
    sys.exit(1)


def load_design_space(csv_path: str) -> tuple[pd.DataFrame, list[str]]:
    """åŠ è½½è®¾è®¡ç©ºé—´å¹¶æå–å› å­åˆ—"""
    df = pd.read_csv(csv_path)
    print(f"\nŠ è®¾è®¡ç©ºé—´: {Path(csv_path).name}")
    print(f"   æ€»è¡Œæ•°: {len(df)}")
    print(f"   åˆ—: {list(df.columns)}")

    # æ™ºèƒ½è¯†åˆ« x1, x2, ... xN æ ¼å¼çš„å› å­åˆ—
    import re
    factor_cols = [c for c in df.columns if re.match(r'^x\d+', c)]

    if not factor_cols:
        print("[è­¦å‘Š] æœªæ‰¾åˆ° x1, x2... æ ¼å¼çš„åˆ—ï¼Œä½¿ç”¨æ‰€æœ‰åˆ—")
        factor_cols = list(df.columns)
    else:
        excluded = set(df.columns) - set(factor_cols)
        if excluded:
            print(f"   âœ… æ™ºèƒ½è¿‡æ»¤: ä½¿ç”¨ {len(factor_cols)} ä¸ªå› å­åˆ—")
            print(f"   âŒ æ’é™¤çš„åˆ—: {', '.join(sorted(excluded))}")

    return df, factor_cols


def encode_design_space(df: pd.DataFrame, factor_cols: list[str]) -> tuple[np.ndarray, dict]:
    """ç¼–ç è®¾è®¡ç©ºé—´ (åˆ†ç±»å˜é‡ -> æ•°å€¼)"""
    df_factors = df[factor_cols].copy()
    encodings = {}

    for col in df_factors.columns:
        if df_factors[col].dtype == 'object':
            unique_vals = sorted(df_factors[col].dropna().unique())
            mapping = {v: i for i, v in enumerate(unique_vals)}
            encodings[col] = mapping
            df_factors[col] = df_factors[col].map(mapping)
        elif df_factors[col].dtype == 'bool':
            df_factors[col] = df_factors[col].astype(int)

    X = df_factors.values.astype(float)
    return X, encodings


def simulate_subjects(
    X: np.ndarray,
    n_subjects: int = 10,
    seed: int = 42,
    population_mean: float = 0.0,
    population_std: float = 0.25,
    individual_std_percent: float = 0.5,
    likert_levels: int = 5,
    likert_mode: str = "tanh",
    likert_sensitivity: float = 2.0,
    interaction_pairs: list = None,
    interaction_scale: float = 0.25,
) -> tuple[np.ndarray, list]:
    """æ¨¡æ‹Ÿå¤šä¸ªè¢«è¯•çš„å“åº” (ä½¿ç”¨v2 LinearSubject)

    è¿”å›:
        responses: (n_subjects, n_trials) å“åº”çŸ©é˜µ
        subjects: è¢«è¯•å¯¹è±¡åˆ—è¡¨
    """
    n_trials = X.shape[0]
    n_features = X.shape[1]

    print(f"\n‘¬ æ¨¡æ‹Ÿ {n_subjects} ä¸ªè¢«è¯•...")
    print(f"   è¯•æ¬¡æ•°: {n_trials}")
    print(f"   ç‰¹å¾æ•°: {n_features}")
    print(f"   ç¾¤ä½“åˆ†å¸ƒ: N({population_mean}, {population_std}Â²)")
    print(f"   ä¸ªä½“åå·®: {individual_std_percent} Ã— {population_std} = {individual_std_percent * population_std}")
    print(f"   Likert: {likert_levels}çº§, mode={likert_mode}, sensitivity={likert_sensitivity}")

    # ç”Ÿæˆç¾¤ä½“æƒé‡ (å…±äº«)
    np.random.seed(seed)
    population_weights = np.random.normal(
        population_mean, population_std, size=(n_features,)
    )

    # æ„å»ºäº¤äº’æƒé‡å­—å…¸
    interaction_weights_dict = None
    if interaction_pairs:
        n_interactions = len(interaction_pairs)
        interaction_weights_array = np.random.normal(
            0.0, interaction_scale, size=(n_interactions,)
        )
        # è½¬æ¢ä¸º {(i,j): weight} å­—å…¸æ ¼å¼
        interaction_weights_dict = {
            tuple(pair): float(weight)
            for pair, weight in zip(interaction_pairs, interaction_weights_array)
        }
        print(f"   äº¤äº’é¡¹: {len(interaction_pairs)} å¯¹, scale={interaction_scale}")

    # åˆ›å»ºè¢«è¯•é›†ç¾¤ç”Ÿæˆå™¨
    # æ³¨æ„: ClusterGeneratoréœ€è¦design_spaceå‚æ•°,æˆ‘ä»¬è¿™é‡Œç®€åŒ–å¤„ç†
    # ç›´æ¥ç”Ÿæˆä¸ªä½“æƒé‡
    subjects = []
    responses = np.zeros((n_subjects, n_trials))

    for subj_idx in range(n_subjects):
        # ä¸ºæ¯ä¸ªè¢«è¯•ç”Ÿæˆä¸ªä½“æƒé‡
        np.random.seed(seed + subj_idx)
        individual_weights = np.random.normal(
            population_mean,
            np.sqrt(population_std**2 + (population_std * individual_std_percent)**2),
            size=(n_features,)
        )

        # åˆ›å»ºLinearSubject
        subject = LinearSubject(
            weights=individual_weights,
            interaction_weights=interaction_weights_dict,
            bias=0.0,
            noise_std=0.0,  # è¯•æ¬¡å†…æ— é¢å¤–å™ªå£°
            likert_levels=likert_levels,
            likert_sensitivity=likert_sensitivity,
            seed=seed + subj_idx,
        )
        subjects.append(subject)

        # å¯¹æ¯ä¸ªè¯•æ¬¡ç”Ÿæˆå“åº”
        for trial_idx in range(n_trials):
            x = X[trial_idx, :]
            y = subject(x)
            responses[subj_idx, trial_idx] = y

    print(f"   âœ… æ¨¡æ‹Ÿå®Œæˆ")
    return responses, subjects


def analyze_distribution(responses: np.ndarray, likert_levels: int = 5) -> dict:
    """åˆ†æå“åº”åˆ†å¸ƒç»Ÿè®¡ç‰¹æ€§"""
    all_responses = responses.flatten()

    # åŸºç¡€ç»Ÿè®¡
    stats = {
        "mean": float(np.mean(all_responses)),
        "std": float(np.std(all_responses)),
        "median": float(np.median(all_responses)),
        "min": float(np.min(all_responses)),
        "max": float(np.max(all_responses)),
        "skewness": float(_skewness(all_responses)),
        "kurtosis": float(_kurtosis(all_responses)),
    }

    # Likertåˆ†å¸ƒ
    if likert_levels > 0:
        counter = Counter(all_responses)
        likert_dist = {int(k): int(v) for k, v in sorted(counter.items())}
        stats["likert_distribution"] = likert_dist

        # è®¡ç®—ç™¾åˆ†æ¯”
        total = len(all_responses)
        likert_percent = {k: v/total*100 for k, v in likert_dist.items()}
        stats["likert_percent"] = {k: round(v, 2) for k, v in likert_percent.items()}

    # è¢«è¯•é—´å·®å¼‚
    subject_means = np.mean(responses, axis=1)
    stats["between_subject_std"] = float(np.std(subject_means))
    stats["between_subject_range"] = float(np.max(subject_means) - np.min(subject_means))

    return stats


def _skewness(x: np.ndarray) -> float:
    """è®¡ç®—ååº¦"""
    x = x - np.mean(x)
    m3 = np.mean(x**3)
    m2 = np.mean(x**2)
    return m3 / (m2**1.5 + 1e-10)


def _kurtosis(x: np.ndarray) -> float:
    """è®¡ç®—å³°åº¦ (excess kurtosis)"""
    x = x - np.mean(x)
    m4 = np.mean(x**4)
    m2 = np.mean(x**2)
    return m4 / (m2**2 + 1e-10) - 3


def print_report(stats: dict, output_path: Path = None):
    """æ‰“å°åˆ†ææŠ¥å‘Š"""
    print("\n" + "="*80)
    print("ˆ å“åº”åˆ†å¸ƒåˆ†ææŠ¥å‘Š")
    print("="*80)

    print("\nŠ åŸºç¡€ç»Ÿè®¡:")
    print(f"   å‡å€¼:   {stats['mean']:.3f}")
    print(f"   æ ‡å‡†å·®: {stats['std']:.3f}")
    print(f"   ä¸­ä½æ•°: {stats['median']:.3f}")
    print(f"   èŒƒå›´:   [{stats['min']:.1f}, {stats['max']:.1f}]")

    print("\n åˆ†å¸ƒå½¢çŠ¶:")
    print(f"   ååº¦ (Skewness): {stats['skewness']:.3f}")
    skew_interp = "å·¦å" if stats['skewness'] < -0.5 else ("å³å" if stats['skewness'] > 0.5 else "å¯¹ç§°")
    print(f"      â†’ {skew_interp}åˆ†å¸ƒ")

    print(f"   å³°åº¦ (Kurtosis):  {stats['kurtosis']:.3f}")
    kurt_interp = "åšå°¾" if stats['kurtosis'] > 0 else "è–„å°¾"
    print(f"      â†’ {kurt_interp}åˆ†å¸ƒ")

    if "likert_distribution" in stats:
        print("\n¯ Likertåˆ†å¸ƒ:")
        for level in sorted(stats["likert_distribution"].keys()):
            count = stats["likert_distribution"][level]
            percent = stats["likert_percent"][level]
            bar_length = int(percent / 2)  # æ¯ä¸ª#ä»£è¡¨2%
            bar = "â–ˆ" * bar_length
            print(f"   {level}: {bar} {percent:.1f}% (n={count})")

    print("\n¥ è¢«è¯•é—´å·®å¼‚:")
    print(f"   è¢«è¯•å‡å€¼çš„æ ‡å‡†å·®: {stats['between_subject_std']:.3f}")
    print(f"   è¢«è¯•å‡å€¼çš„èŒƒå›´:   {stats['between_subject_range']:.3f}")

    print("\nâœ… åˆ†å¸ƒè¯„ä¼°:")
    # è¯„ä¼°æ­£æ€æ€§
    is_symmetric = abs(stats['skewness']) < 0.5
    is_normal_kurt = abs(stats['kurtosis']) < 1.0
    print(f"   å¯¹ç§°æ€§: {'âœ… é€šè¿‡' if is_symmetric else 'âŒ åæ–œ'}")
    print(f"   å³°åº¦æ­£å¸¸: {'âœ… é€šè¿‡' if is_normal_kurt else 'âŒ å¼‚å¸¸'}")

    # è¯„ä¼°Likertåˆ†å¸ƒåˆç†æ€§ (ä¸­é—´å€¼åº”è¯¥å¤šï¼Œæç«¯å€¼å°‘)
    if "likert_percent" in stats:
        middle = stats["likert_percent"].get(3, 0)  # å‡è®¾5çº§é‡è¡¨
        extremes = stats["likert_percent"].get(1, 0) + stats["likert_percent"].get(5, 0)
        is_reasonable = middle > extremes
        print(f"   Likertåˆç†æ€§: {'âœ… ä¸­é—´å€¼å¤šäºæç«¯å€¼' if is_reasonable else 'âŒ æç«¯å€¼è¿‡å¤š'}")

    print("\n" + "="*80)

    # ä¿å­˜JSON
    if output_path:
        output_path.mkdir(parents=True, exist_ok=True)
        json_path = output_path / "distribution_stats.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        print(f"\n¾ ç»Ÿè®¡ç»“æœå·²ä¿å­˜: {json_path}")


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    design_csv = str(Path(__file__).resolve().parents[3] / "data" / "i9csy65bljq14ovww2v91-6532622b_JBmIu2QSKA.csv")
    output_dir = Path(__file__).parent / "output" / "distribution_test"

    config = {
        "n_subjects": 20,  # æµ‹è¯•20ä¸ªè¢«è¯•
        "seed": 42,
        "population_mean": 0.0,
        "population_std": 0.25,
        "individual_std_percent": 0.5,
        "likert_levels": 5,
        "likert_mode": "tanh",
        "likert_sensitivity": 2.0,
        "interaction_pairs": [(3, 4), (0, 1)],
        "interaction_scale": 0.25,
    }

    print("\n" + "="*80)
    print("[TEST] å“åº”åˆ†å¸ƒæµ‹è¯•")
    print("="*80)

    # 1. åŠ è½½è®¾è®¡ç©ºé—´
    df, factor_cols = load_design_space(design_csv)

    # 2. ç¼–ç 
    X, encodings = encode_design_space(df, factor_cols)
    if encodings:
        print(f"\n[ENCODE] ç¼–ç çš„åˆ—: {list(encodings.keys())}")

    # 3. æ¨¡æ‹Ÿè¢«è¯•
    responses, subjects = simulate_subjects(X, **config)

    # 4. åˆ†æåˆ†å¸ƒ
    stats = analyze_distribution(responses, config["likert_levels"])

    # 5. æ‰“å°æŠ¥å‘Š
    print_report(stats, output_dir)

    # 6. ä¿å­˜å“åº”æ•°æ®æ ·æœ¬
    sample_df = df[factor_cols].head(10).copy()
    for i in range(min(5, config["n_subjects"])):
        sample_df[f"subject_{i+1}"] = responses[i, :10]

    sample_csv = output_dir / "sample_responses.csv"
    sample_df.to_csv(sample_csv, index=False)
    print(f"[SAVE] å“åº”æ ·æœ¬å·²ä¿å­˜: {sample_csv}")

    print("\n[DONE] æµ‹è¯•å®Œæˆ!\n")


if __name__ == "__main__":
    main()
