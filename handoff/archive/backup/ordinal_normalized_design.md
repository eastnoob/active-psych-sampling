# Ordinalå‚æ•°å®ç°ï¼šè§„èŒƒåŒ–å€¼æ–¹æ¡ˆ

**åŸºäºè¯„å®¡åé¦ˆçš„ä¿®è®¢è®¾è®¡**
**æ—¥æœŸ**: 2025-12-11

---

## æ ¸å¿ƒè®¾è®¡åŸåˆ™

### åŸåˆ™1: Transformäº§ç”Ÿä¿ç•™é—´è·çš„è§„èŒƒåŒ–å€¼

```
ç‰©ç†å€¼ç©ºé—´          è§„èŒƒåŒ–å€¼ç©ºé—´ (æ¨¡å‹è¾“å…¥)      å†…éƒ¨Rankç©ºé—´ (å¯é€‰)
[2.0, 2.5, 3.5]  â†’  [0.0, 0.25, 1.0]      â†’  [0, 1, 2]
   â†‘                      â†‘                        â†‘
 ç”¨æˆ·é…ç½®            GP/ANOVAçœ‹åˆ°çš„å€¼        ç¦»æ•£çº¦æŸç”¨
```

**å…³é”®**: æ¨¡å‹ç©ºé—´ä½¿ç”¨ `[0.0, 0.25, 1.0]` è€Œé `[0, 1, 2]`

### åŸåˆ™2: LocalSampleråœ¨è§„èŒƒåŒ–å€¼ç©ºé—´æ‰°åŠ¨

- `base[:,:,k]` åŒ…å«è§„èŒƒåŒ–å€¼ (e.g., 0.25)
- é«˜æ–¯æ‰°åŠ¨: `0.25 + N(0, Ïƒ)` â†’ æœ€è¿‘é‚»çº¦æŸ â†’ `{0.0, 0.25, 1.0}`
- æ— éœ€Transformå¯¹è±¡ï¼Œåªéœ€çŸ¥é“æœ‰æ•ˆçš„è§„èŒƒåŒ–å€¼åˆ—è¡¨

### åŸåˆ™3: ANOVAçœ‹åˆ°æ­£ç¡®é—´è·

å½“åˆ†è§£ä¸»æ•ˆåº”æ—¶:
```python
# å‚æ•°æ•ˆåº”è®¡ç®—
effect_of_height = model.predict([0.0, ...]) vs model.predict([0.25, ...]) vs model.predict([1.0, ...])
# ANOVAçœ‹åˆ°ï¼š0â†’0.25 (å°å¢é‡) vs 0.25â†’1.0 (å¤§å¢é‡)
# è¿™æ­£ç¡®åæ˜ äº† 2.0â†’2.5 (0.5m) vs 2.5â†’3.5 (1.0m) çš„ç‰©ç†å…³ç³»
```

---

## Ordinal Transformå®ç°

### æ ¸å¿ƒç±»è®¾è®¡

```python
class Ordinal(Transform, StringParameterMixin):
    """æœ‰åºå‚æ•°Transform - è¾“å‡ºä¿ç•™é—´è·çš„è§„èŒƒåŒ–å€¼"""

    def __init__(
        self,
        indices: list[int],
        values: dict[int, list[float]],  # {0: [2.0, 2.5, 3.5]}
        level_names: Optional[dict[int, list[str]]] = None,
    ):
        super().__init__()
        self.indices = indices
        self.values = values  # åŸå§‹ç‰©ç†å€¼
        self.level_names = level_names

        # è®¡ç®—è§„èŒƒåŒ–æ˜ å°„
        self._build_normalized_mappings()

    def _build_normalized_mappings(self):
        """æ„å»ºç‰©ç†å€¼ â†” è§„èŒƒåŒ–å€¼çš„åŒå‘æ˜ å°„"""
        self.normalized_values = {}  # {index: [norm_v0, norm_v1, ...]}
        self.physical_to_normalized = {}  # {index: {phys_val: norm_val}}
        self.normalized_to_physical = {}  # {index: {norm_val: phys_val}}

        for idx in self.indices:
            phys_vals = np.array(self.values[idx], dtype=np.float64)

            # Min-maxå½’ä¸€åŒ–åˆ°[0, 1]
            min_val = phys_vals.min()
            max_val = phys_vals.max()

            if max_val - min_val < 1e-10:
                # æ‰€æœ‰å€¼ç›¸åŒï¼Œå½’ä¸€åŒ–ä¸º0
                norm_vals = np.zeros_like(phys_vals)
            else:
                norm_vals = (phys_vals - min_val) / (max_val - min_val)

            # ä¿å­˜æ˜ å°„
            self.normalized_values[idx] = norm_vals

            # æ„å»ºåŒå‘å­—å…¸ (å¤„ç†æµ®ç‚¹ç²¾åº¦)
            self.physical_to_normalized[idx] = {
                round(p, 10): round(n, 10)
                for p, n in zip(phys_vals, norm_vals)
            }
            self.normalized_to_physical[idx] = {
                round(n, 10): round(p, 10)
                for n, p in zip(norm_vals, phys_vals)
            }

    @subset_transform
    def _transform(self, X: torch.Tensor) -> torch.Tensor:
        """ç‰©ç†å€¼ â†’ è§„èŒƒåŒ–å€¼

        è¾“å…¥: [[2.5], [3.5], [2.0]]
        è¾“å‡º: [[0.25], [1.0], [0.0]]
        """
        X_normalized = X.clone()

        for i, idx in enumerate(self.indices):
            phys_vals = X[..., i].cpu().numpy()
            norm_vals = np.zeros_like(phys_vals)

            # æŸ¥è¡¨è½¬æ¢
            phys_to_norm = self.physical_to_normalized[idx]
            for j, pv in enumerate(phys_vals.flat):
                pv_rounded = round(pv, 10)
                if pv_rounded not in phys_to_norm:
                    # æœ€è¿‘é‚»åŒ¹é… (å®¹é”™)
                    closest = min(phys_to_norm.keys(), key=lambda x: abs(x - pv_rounded))
                    norm_vals.flat[j] = phys_to_norm[closest]
                else:
                    norm_vals.flat[j] = phys_to_norm[pv_rounded]

            X_normalized[..., i] = torch.from_numpy(norm_vals).to(dtype=X.dtype)

        return X_normalized

    @subset_transform
    def _untransform(self, X: torch.Tensor) -> torch.Tensor:
        """è§„èŒƒåŒ–å€¼ â†’ ç‰©ç†å€¼

        è¾“å…¥: [[0.25], [1.0], [0.0]]
        è¾“å‡º: [[2.5], [3.5], [2.0]]
        """
        X_physical = X.clone()

        for i, idx in enumerate(self.indices):
            norm_vals = X[..., i].cpu().numpy()
            phys_vals = np.zeros_like(norm_vals)

            # æŸ¥è¡¨è½¬æ¢
            norm_to_phys = self.normalized_to_physical[idx]
            for j, nv in enumerate(norm_vals.flat):
                nv_rounded = round(nv, 10)
                if nv_rounded not in norm_to_phys:
                    # æœ€è¿‘é‚»åŒ¹é…
                    closest = min(norm_to_phys.keys(), key=lambda x: abs(x - nv_rounded))
                    phys_vals.flat[j] = norm_to_phys[closest]
                else:
                    phys_vals.flat[j] = norm_to_phys[nv_rounded]

            X_physical[..., i] = torch.from_numpy(phys_vals).to(dtype=X.dtype)

        return X_physical

    def transform_bounds(
        self,
        X: torch.Tensor,
        bound: Literal["lb", "ub"] | None = None,
        epsilon: float = 1e-6
    ) -> torch.Tensor:
        """ç‰©ç†è¾¹ç•Œ â†’ è§„èŒƒåŒ–è¾¹ç•Œ

        è¾“å…¥: [[2.0], [3.5]] (ç‰©ç†å€¼)
        è¾“å‡º: [[-Îµ], [1.0+Îµ]] (è§„èŒƒåŒ–å€¼ï¼ŒåŠ å°åç§»ä¿è¯è¦†ç›–)
        """
        X_bounds = X.clone()

        for i, idx in enumerate(self.indices):
            # è§„èŒƒåŒ–åçš„è¾¹ç•Œæ€»æ˜¯[0, 1]
            if bound == "lb":
                X_bounds[0, i] = -epsilon  # ä¸‹ç•Œç¨å¾®æ‰©å±•
            elif bound == "ub":
                X_bounds[0, i] = 1.0 + epsilon  # ä¸Šç•Œç¨å¾®æ‰©å±•
            else:  # both bounds
                X_bounds[0, i] = -epsilon
                X_bounds[1, i] = 1.0 + epsilon

        return X_bounds

    @classmethod
    def get_config_options(
        cls,
        config: Config,
        name: str,
        options: dict = None
    ) -> dict:
        """ä»é…ç½®è§£æordinalå‚æ•°"""
        options = options or {}

        # ä¼˜å…ˆçº§1: ç›´æ¥æŒ‡å®švalues
        if "values" in options:
            values = options["values"]
            return {"indices": [0], "values": {0: list(values)}}

        # ä¼˜å…ˆçº§2: min/max + step
        if "min_value" in options and "max_value" in options:
            min_val = float(options["min_value"])
            max_val = float(options["max_value"])

            if "step" in options:
                step = float(options["step"])
                # ä½¿ç”¨linspaceé¿å…ç´¯ç§¯è¯¯å·®
                num_steps = int(round((max_val - min_val) / step)) + 1
                values = np.linspace(min_val, max_val, num_steps)
            elif "num_levels" in options:
                num_levels = int(options["num_levels"])
                values = np.linspace(min_val, max_val, num_levels)
            else:
                raise ValueError(
                    f"[{name}] Must specify 'step' or 'num_levels' with min/max_value"
                )

            return {"indices": [0], "values": {0: list(values)}}

        # ä¼˜å…ˆçº§3: levels (å­—ç¬¦ä¸²æ ‡ç­¾)
        if "levels" in options:
            levels = options["levels"]
            if isinstance(levels, str):
                levels = [s.strip() for s in levels.split(',')]

            # å­—ç¬¦ä¸²æ ‡ç­¾ â†’ æ•´æ•°åºåˆ— (ç­‰å·®)
            values = list(range(len(levels)))
            return {
                "indices": [0],
                "values": {0: values},
                "level_names": {0: levels}
            }

        raise ValueError(
            f"[{name}] Must specify one of:\n"
            "  1. 'values' (direct list)\n"
            "  2. 'min_value' + 'max_value' + ('step' or 'num_levels')\n"
            "  3. 'levels' (string labels)"
        )
```

---

## ğŸš¨ å…³é”®æ•°æ®æµï¼šunique_vals_dictçš„åˆå§‹åŒ–

### é—®é¢˜åˆ†æ

**æ ¸å¿ƒé—®é¢˜**: LocalSamplerçš„ `_unique_vals_dict` éœ€è¦åŒ…å«è§„èŒƒåŒ–å€¼ `[0.0, 0.333, 1.0]`ï¼Œä½†è¿™ä¸ªæ•°æ®ä»å“ªé‡Œæ¥ï¼Ÿ

**æ•°æ®æµè¿½è¸ª**:

```python
# 1. Transformå±‚ç”Ÿæˆè§„èŒƒåŒ–å€¼
ordinal_transform = Ordinal(...)
ordinal_transform.normalized_values = {0: [0.0, 0.333, 1.0]}

# 2. Poolç”Ÿæˆä½¿ç”¨è§„èŒƒåŒ–å€¼
pool = [[0.0], [0.333], [1.0]]  # âœ“ åŒ…å«è§„èŒƒåŒ–å€¼

# 3. LocalSampleråˆå§‹åŒ– â† ğŸš¨ ç¼ºå¤±ç¯èŠ‚ï¼
local_sampler = LocalSampler(
    variable_types={0: 'ordinal'},
    unique_vals_dict={0: ???}  # â† ä»å“ªé‡Œè·å– [0.0, 0.333, 1.0]ï¼Ÿ
)
```

### è§£å†³æ–¹æ¡ˆï¼šä»Poolç›´æ¥æå–ï¼ˆæœ€ç®€æ–¹æ¡ˆï¼‰

**æ ¸å¿ƒæ´å¯Ÿ**: Poolå·²ç»åŒ…å«äº†æ­£ç¡®çš„è§„èŒƒåŒ–å€¼ï¼Œç›´æ¥ä»poolæå–å³å¯ï¼

#### ä¿®æ”¹ï¼šLocalSampleråˆå§‹åŒ–æ”¯æŒä»poolè‡ªåŠ¨æå–

```python
class LocalSampler:
    def __init__(
        self,
        local_num: int,
        local_jitter_frac: float,
        variable_types: Dict[int, str],
        pool: torch.Tensor = None,  # â† æ–°å¢poolå‚æ•°
        unique_vals_dict: Dict[int, np.ndarray] = None,
        use_hybrid_perturbation: bool = False,
        ...
    ):
        # ä¼˜å…ˆä½¿ç”¨æ˜¾å¼æä¾›çš„unique_vals_dict
        if unique_vals_dict is not None:
            self._unique_vals_dict = unique_vals_dict
        elif pool is not None:
            # ğŸ”‘ å…³é”®ï¼šä»poolè‡ªåŠ¨æå–uniqueå€¼
            self._unique_vals_dict = self._extract_unique_vals_from_pool(
                pool, variable_types
            )
        else:
            self._unique_vals_dict = {}
            warnings.warn(
                "LocalSampler initialized without pool or unique_vals_dict. "
                "Ordinal/categorical perturbation may not work correctly."
            )

    @staticmethod
    def _extract_unique_vals_from_pool(
        pool: torch.Tensor,
        variable_types: Dict[int, str]
    ) -> Dict[int, np.ndarray]:
        """ä»poolæå–ordinal/categoricalçš„uniqueå€¼

        Args:
            pool: å€™é€‰ç‚¹pool (å·²ç»åœ¨è§„èŒƒåŒ–å€¼ç©ºé—´)
            variable_types: å˜é‡ç±»å‹å­—å…¸

        Returns:
            unique_vals_dict: {dim_idx: unique_values_array}
        """
        unique_vals_dict = {}

        for k in range(pool.shape[1]):
            vt = variable_types.get(k)

            if vt in ["ordinal", "custom_ordinal", "custom_ordinal_mono"]:
                # Ordinal: éœ€è¦æ’åºä»¥ä¾¿æœ€è¿‘é‚»æŸ¥æ‰¾æ­£ç¡®å·¥ä½œ
                # torch.unique()è‡ªåŠ¨æ’åºï¼Œä½†æˆ‘ä»¬æ˜¾å¼è°ƒç”¨np.sort()ç¡®ä¿æ„å›¾æ¸…æ™°
                unique_vals = torch.unique(pool[:, k]).cpu().numpy()
                unique_vals = np.sort(unique_vals)  # ç¡®ä¿å‡åºæ’åˆ—
                unique_vals_dict[k] = unique_vals

                logger.debug(
                    f"[LocalSampler] Extracted {len(unique_vals)} unique ordinal values "
                    f"for dimension {k}: {unique_vals} (sorted)"
                )

            elif vt == "categorical":
                # Categorical: é¡ºåºä¸é‡è¦ï¼Œä½†ä¿æŒä¸€è‡´æ€§ä¹Ÿæ’åº
                unique_vals = torch.unique(pool[:, k]).cpu().numpy()
                unique_vals_dict[k] = unique_vals  # ä¸éœ€è¦é¢å¤–æ’åº

                logger.debug(
                    f"[LocalSampler] Extracted {len(unique_vals)} unique categorical values "
                    f"for dimension {k}: {unique_vals}"
                )

        return unique_vals_dict
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
# åœ¨EURAnovaPairAcqfåˆå§‹åŒ–LocalSampler
class EURAnovaPairAcqf:
    def __init__(self, model, pool, ...):
        # è§£æå˜é‡ç±»å‹ï¼ˆä¿æŒç°æœ‰é€»è¾‘ï¼‰
        variable_types = self._infer_variable_types_from_transforms(model.transforms)

        # åˆå§‹åŒ–LocalSampler - è‡ªåŠ¨ä»poolæå–uniqueå€¼
        self.local_sampler = LocalSampler(
            local_num=self.local_num,
            local_jitter_frac=self.local_jitter_frac,
            variable_types=variable_types,
            pool=pool,  # â† åªéœ€ä¼ å…¥poolï¼Œè‡ªåŠ¨æå–ï¼
            use_hybrid_perturbation=self.use_hybrid_perturbation,
            ...
        )
```

#### Advantages of Pool-Based Extraction

**Principle 1: Simplicity**
- Implementation: 3 core lines of logic in `_extract_unique_vals_from_pool()`
- No new config_parser functions required
- Single responsibility: LocalSampler extracts what it needs from pool

**Principle 2: Data Consistency**
- Pool serves as single source of truth for normalized values
- Avoids synchronization issues between Transform and Pool
- Direct extraction ensures what model sees matches what LocalSampler perturbs

**Principle 3: Zero Breaking Changes**
- Pool parameter is optional in LocalSampler.__init__()
- Existing code paths remain unchanged
- Backward compatible with manual unique_vals_dict provision

**Principle 4: Automatic Operation**
- Pool already contains normalized values from Transform
- No additional Transform object dependency in LocalSampler
- Extraction happens transparently during initialization

**Principle 5: Architectural Alignment**
- Follows EUR's existing pattern: Pool â†’ LocalSampler
- Matches categorical/integer handling philosophy
- Maintains separation of concerns: Transform for conversion, Pool for candidates, LocalSampler for perturbation

### Complete Data Flow: Pool Extraction Approach

**Overview**: This design combines the best of both approaches - Transform handles normalization, Pool serves as single source of truth, and LocalSampler extracts directly from Pool.

```yaml
Stage_1_Configuration_Parsing:
  input:
    - config_file: "[height] par_type=custom_ordinal, values=[2.0, 2.5, 3.5]"
  process:
    - action: "Initialize Ordinal Transform"
    - computation: "min_max_normalization"
    - formula: "(value - min) / (max - min)"
    - example: "(2.0-2.0)/(3.5-2.0)=0.0, (2.5-2.0)/1.5=0.333, (3.5-2.0)/1.5=1.0"
  output:
    - transform_object: "Ordinal"
    - normalized_values: "[0.0, 0.333, 1.0]"
    - physical_to_normalized_map: "{2.0: 0.0, 2.5: 0.333, 3.5: 1.0}"

Stage_2_Pool_Generation:
  input:
    - ordinal_transform: "from Stage_1"
  process:
    - function: "CustomPoolBasedGenerator._generate_pool_from_config()"
    - action: "Extract normalized_values from Transform.normalized_values[0]"
    - note: "Pool stores normalized values, NOT physical values"
  output:
    - pool_tensor: "torch.tensor([[0.0], [0.333], [1.0]])"
    - data_type: "float (normalized)"
    - interpretation: "Each pool point is already in model input space"

Stage_3_LocalSampler_Initialization:
  design_choice: "Pool-based extraction (NEW)"
  rationale:
    - pool_already_contains: "normalized values from Stage_2"
    - no_transform_dependency: "LocalSampler doesn't need Transform object"
    - single_source_of_truth: "Pool is authoritative"
  input:
    - pool: "from Stage_2"
    - variable_types: "{0: 'ordinal'}"
  process:
    - function: "LocalSampler.__init__(pool=pool, variable_types={0: 'ordinal'})"
    - conditional_check:
        if_unique_vals_dict_provided:
          action: "Use provided dict directly"
        elif_pool_provided:
          action: "Call _extract_unique_vals_from_pool()"
          implementation: "torch.unique(pool[:, 0]).cpu().numpy()"
          result: "Extract [0.0, 0.333, 1.0] from pool"
        else:
          action: "Warn and use empty dict"
  output:
    - local_sampler_object: "LocalSampler"
    - internal_state: "_unique_vals_dict = {0: np.array([0.0, 0.333, 1.0])}"
    - advantage: "Direct extraction, zero Transform dependency"

Stage_4_Perturbation:
  input:
    - candidate_point: "torch.tensor([[0.333]])  # normalized value"
    - unique_vals_dict: "from Stage_3: {0: [0.0, 0.333, 1.0]}"
  process:
    - function: "_perturb_ordinal(base, k=0, B)"
    - step_1_retrieve: "unique_vals = self._unique_vals_dict[0]  # [0.0, 0.333, 1.0]"
    - step_2_perturb:
        method: "Gaussian noise in normalized space"
        formula: "perturbed = 0.333 + N(0, sigma)"
        sigma: "0.1 * 1.0 = 0.1  # 10% of normalized range"
        example_samples: "[0.283, 0.453, 0.253, ...]"
    - step_3_constrain:
        method: "Nearest neighbor to valid values"
        implementation: "np.argmin(np.abs(unique_vals - perturbed_value))"
        example_mapping:
          - "0.283 â†’ 0.333 (closest)"
          - "0.453 â†’ 0.333 (distance 0.12 to 0.333, distance 0.547 to 1.0)"
          - "0.253 â†’ 0.333 (closest)"
  output:
    - perturbed_samples: "[0.333, 0.333, 0.333, 1.0, 0.0, ...]  # valid normalized values"
    - guarantee: "All outputs are in normalized space and match pool values"

Key_Design_Insight:
  data_flow: "Transform â†’ Pool â†’ LocalSampler"
  extraction_point: "Stage_3 extracts from Pool (Stage_2 output)"
  not_extraction_point: "Stage_3 does NOT go back to Transform (Stage_1)"
  reason: "Pool is single source of truth after Stage_2"
  benefit: "Simpler dependency graph, automatic synchronization"
```

**Critical Implementation Note**: The `_extract_unique_vals_from_pool()` method is called during LocalSampler initialization, making the extraction automatic and transparent. Users only need to pass the pool parameter.

---

## LocalSampleré›†æˆ

### _perturb_ordinalå®ç°

```python
def _perturb_ordinal(
    self,
    base: torch.Tensor,
    k: int,
    B: int
) -> torch.Tensor:
    """æœ‰åºå‚æ•°æ‰°åŠ¨ï¼šåœ¨è§„èŒƒåŒ–å€¼ç©ºé—´æ‰°åŠ¨ + æœ€è¿‘é‚»çº¦æŸ

    å‡è®¾: base[:,:,k] åŒ…å«è§„èŒƒåŒ–å€¼ (e.g., [0.0, 0.25, 1.0])
    """
    # è·å–æœ‰æ•ˆçš„è§„èŒƒåŒ–å€¼åˆ—è¡¨
    unique_normalized_vals = self._unique_vals_dict.get(k)

    if unique_normalized_vals is None or len(unique_normalized_vals) == 0:
        warnings.warn(f"Ordinal dimension {k}: no unique values found, keeping original")
        return base

    unique_vals = np.array(unique_normalized_vals, dtype=np.float64)
    n_levels = len(unique_vals)

    # æ··åˆç­–ç•¥
    if (self.use_hybrid_perturbation and
        n_levels <= self.exhaustive_level_threshold):
        # ========== ç©·ä¸¾æ¨¡å¼ ==========
        if self.exhaustive_use_cyclic_fill:
            n_repeats = (self.local_num // n_levels) + 1
            samples = np.tile(unique_vals, (B, n_repeats))
            samples = samples[:, :self.local_num]
        else:
            samples = np.tile(unique_vals, (B, 1))

        base[:, :samples.shape[1], k] = torch.from_numpy(samples).to(
            dtype=base.dtype, device=base.device
        )
    else:
        # ========== é«˜æ–¯æ‰°åŠ¨æ¨¡å¼ ==========
        # è§„èŒƒåŒ–å€¼ç©ºé—´çš„spanæ€»æ˜¯1.0 (å› ä¸ºå·²å½’ä¸€åŒ–åˆ°[0,1])
        span = 1.0
        sigma = self.local_jitter_frac * span  # e.g., 0.1 * 1.0 = 0.1

        # åœ¨è§„èŒƒåŒ–å€¼ç©ºé—´æ‰°åŠ¨
        center_vals = base[:, :, k].cpu().numpy()  # (B, local_num)
        noise = self._np_rng.normal(0, sigma, size=(B, self.local_num))
        perturbed = center_vals + noise

        # çº¦æŸåˆ°æœ€è¿‘çš„æœ‰æ•ˆè§„èŒƒåŒ–å€¼
        samples = np.zeros_like(perturbed)
        for i in range(B):
            for j in range(self.local_num):
                closest_idx = np.argmin(np.abs(unique_vals - perturbed[i, j]))
                samples[i, j] = unique_vals[closest_idx]

        base[:, :, k] = torch.from_numpy(samples).to(
            dtype=base.dtype, device=base.device
        )

    return base
```

### sample()æ–¹æ³•é›†æˆ

```python
def sample(self, X_can_t: torch.Tensor, dims: Sequence[int]) -> torch.Tensor:
    """ç”Ÿæˆå±€éƒ¨æ‰°åŠ¨ç‚¹"""
    B, d = X_can_t.shape
    base = X_can_t.unsqueeze(1).expand(-1, self.local_num, -1)

    for k in dims:
        vt = self.variable_types.get(k) if self.variable_types else None

        if vt == "categorical":
            base = self._perturb_categorical(base, k, B)
        elif vt in ["ordinal", "custom_ordinal", "custom_ordinal_mono"]:
            base = self._perturb_ordinal(base, k, B)  # â† æ–°å¢
        elif vt == "integer":
            base = self._perturb_integer(base, k, B, mn[k], mx[k], span[k])
        else:  # continuous
            base = self._perturb_continuous(base, k, B, mn[k], mx[k], span[k])

    return base.reshape(B * self.local_num, d)
```

---

## CustomPoolBasedGeneratoré›†æˆ

### Poolç”Ÿæˆä½¿ç”¨è§„èŒƒåŒ–å€¼

```python
def _generate_pool_from_config(cls, config: Config) -> torch.Tensor:
    """ç”Ÿæˆpoolï¼Œordinalå‚æ•°ä½¿ç”¨è§„èŒƒåŒ–å€¼"""
    param_choices_values = []

    for par_name in parnames:
        par_type = config.get(par_name, "par_type", "continuous")

        if par_type == "categorical":
            choices = config.getlist(par_name, "choices")
            # Categoricalç”¨ç´¢å¼• [0, 1, 2, ...]
            param_choices_values.append(list(range(len(choices))))

        elif par_type in ["custom_ordinal", "custom_ordinal_mono"]:
            from aepsych.transforms.ops.ordinal import Ordinal

            # åˆ›å»ºOrdinal transform
            options = {}
            for key in ["values", "min_value", "max_value", "step", "num_levels", "levels"]:
                if config.has_option(par_name, key):
                    options[key] = config.get(par_name, key)

            ordinal_config = Ordinal.get_config_options(config, par_name, options)
            ordinal = Ordinal(**ordinal_config)

            # ä½¿ç”¨è§„èŒƒåŒ–å€¼
            normalized_vals = ordinal.normalized_values[ordinal.indices[0]]
            param_choices_values.append(list(normalized_vals))

            logger.info(
                f"[PoolGen] Added ordinal param '{par_name}' with {len(normalized_vals)} "
                f"levels (normalized values: {normalized_vals})"
            )

        elif par_type == "integer":
            lb = config.getint(par_name, "lb")
            ub = config.getint(par_name, "ub")
            param_choices_values.append(list(range(lb, ub + 1)))

    # ç”Ÿæˆå®Œæ•´ç»„åˆ
    pool = generate_full_factorial(param_choices_values)
    return pool
```

---

## å·¥ä½œæµç¤ºä¾‹

### ç¤ºä¾‹1: éç­‰å·®ç‰©ç†å‚æ•°

```ini
[height]
par_type = custom_ordinal
values = [2.0, 2.5, 3.5]  # éç­‰å·®
```

**å¤„ç†æµç¨‹**:

```
1. ç”¨æˆ·é…ç½®ç‰©ç†å€¼
   values = [2.0, 2.5, 3.5]

2. Ordinal Transformåˆå§‹åŒ–
   min = 2.0, max = 3.5, span = 1.5
   normalized_values = [(2.0-2.0)/1.5, (2.5-2.0)/1.5, (3.5-2.0)/1.5]
                     = [0.0, 0.333, 1.0]

3. Poolç”Ÿæˆ
   param_choices_values = [[0.0, 0.333, 1.0]]
   pool = torch.tensor([[0.0], [0.333], [1.0]])

4. LocalSampleræ‰°åŠ¨
   base = [[0.333, 0.333, ...]]  # ä¸­å¿ƒç‚¹: è§„èŒƒåŒ–å€¼0.333
   noise = N(0, 0.1)  # Ïƒ = 0.1 * 1.0
   perturbed = 0.333 + [-0.05, 0.12, -0.08, ...]
             = [0.283, 0.453, 0.253, ...]

   æœ€è¿‘é‚»çº¦æŸ:
   0.283 â†’ 0.333 (æœ€è¿‘)
   0.453 â†’ 0.333 (ä¸0.333è·ç¦»0.12ï¼Œä¸1.0è·ç¦»0.547)
   0.253 â†’ 0.333 (æœ€è¿‘)
   ...

   samples = [0.333, 0.333, 0.333, 1.0, 0.0, ...]

5. GPè®­ç»ƒ
   X_trainåŒ…å«è§„èŒƒåŒ–å€¼ [0.0, 0.333, 1.0, ...]
   GPå­¦åˆ°: f(0.0) vs f(0.333) vs f(1.0)

6. ANOVAåˆ†è§£
   ä¸»æ•ˆåº”: Î”(0.0â†’0.333) vs Î”(0.333â†’1.0)
   ANOVAçœ‹åˆ°: 0.333é—´è· vs 0.667é—´è·
   è¿™æ­£ç¡®åæ˜ äº†ç‰©ç†ä¸Š 0.5m vs 1.0m çš„æ¯”ä¾‹å…³ç³»ï¼

7. ç”¨æˆ·æŸ¥è¯¢ç»“æœ
   untransform: [0.333] â†’ [2.5m]
```

### ç¤ºä¾‹2: ç­‰å·®Likerté‡è¡¨

```ini
[agreement]
par_type = custom_ordinal
min_value = 1
max_value = 5
step = 1
```

**å¤„ç†æµç¨‹**:

```
1. é…ç½®è§£æ
   values = [1, 2, 3, 4, 5]

2. è§„èŒƒåŒ–
   normalized_values = [0.0, 0.25, 0.5, 0.75, 1.0]

3. Poolç”Ÿæˆ
   poolåŒ…å« [0.0, 0.25, 0.5, 0.75, 1.0]

4. ANOVAçœ‹åˆ°ç­‰é—´è·
   Î” = 0.25 (æ‰€æœ‰ç›¸é‚»çº§åˆ«é—´è·ç›¸åŒ)
   è¿™ç¬¦åˆLikerté‡è¡¨çš„å¿ƒç†å­¦å‡è®¾
```

---

## ä¼˜åŠ¿æ€»ç»“

### 1. ä¿ç•™ç‰©ç†é—´è·ä¿¡æ¯ âœ…

- ANOVAåˆ†è§£çœ‹åˆ°æ­£ç¡®çš„ç›¸å¯¹é—´è·
- GPå­¦ä¹ æ—¶åˆ©ç”¨é—´è·ç»“æ„
- æ•ˆåº”ä¼°è®¡æ›´å‡†ç¡®

### 2. æ¶æ„ä¸€è‡´æ€§ âœ…

- LocalSampleræ— éœ€ä¿®æ”¹ç­¾å
- ä¸categorical/integerå¤„ç†æ¨¡å¼ç»Ÿä¸€
- æ— ç ´åæ€§å˜æ›´

### 3. å®ç°ç®€æ´ âœ…

- Transformè´Ÿè´£å½’ä¸€åŒ–é€»è¾‘
- LocalSampleråªéœ€æœ€è¿‘é‚»çº¦æŸ
- Poolç”Ÿæˆè‡ªç„¶åŒ…å«è§„èŒƒåŒ–å€¼

### 4. æ•°å­¦åˆç†æ€§ âœ…

- è§„èŒƒåŒ–åˆ°[0,1]æ˜¯æ ‡å‡†åšæ³•
- ä¿ç•™é—´è·æ¯”ä¾‹ä¿¡æ¯
- ç¦»æ•£çº¦æŸé€šè¿‡æœ€è¿‘é‚»å®ç°

### 5. å‘åå…¼å®¹ âœ…

- ç°æœ‰å‚æ•°ç±»å‹ä¸å—å½±å“
- é…ç½®æ ¼å¼ä¿æŒç®€æ´
- APIæ— ç ´åæ€§å˜æ›´

---

## ä¸åŸè®¡åˆ’å¯¹æ¯”

| æ–¹é¢ | åŸè®¡åˆ’ (rankç©ºé—´) | ä¿®è®¢æ–¹æ¡ˆ (è§„èŒƒåŒ–å€¼) |
|------|------------------|--------------------|
| **æ¨¡å‹è¾“å…¥** | [0, 1, 2] (çº¯rank) | [0.0, 0.25, 1.0] (ä¿ç•™é—´è·) |
| **ANOVAé—´è·** | âŒ çœ‹åˆ°ç­‰é—´è·1 | âœ… çœ‹åˆ°çœŸå®æ¯”ä¾‹ |
| **LocalSampler** | rankç©ºé—´æ‰°åŠ¨ | è§„èŒƒåŒ–å€¼ç©ºé—´æ‰°åŠ¨ |
| **Transformå¤æ‚åº¦** | ç®€å• (å€¼â†”rank) | ä¸­ç­‰ (å€¼â†”è§„èŒƒåŒ–å€¼) |
| **ç‰©ç†å«ä¹‰** | âŒ ä¸¢å¤±é—´è·ä¿¡æ¯ | âœ… ä¿ç•™ç›¸å¯¹é—´è· |
| **æ¶æ„ä¸€è‡´æ€§** | âœ… é›¶ç ´åæ€§å˜æ›´ | âœ… é›¶ç ´åæ€§å˜æ›´ |

---

## å®ç°æ£€æŸ¥æ¸…å•

### Phase 1: Ordinal Transform (Day 1)

- [ ] å®ç° `_build_normalized_mappings()`
- [ ] å®ç° `_transform()` (ç‰©ç†å€¼â†’è§„èŒƒåŒ–å€¼)
- [ ] å®ç° `_untransform()` (è§„èŒƒåŒ–å€¼â†’ç‰©ç†å€¼)
- [ ] å®ç° `transform_bounds()` (è§„èŒƒåŒ–è¾¹ç•Œ)
- [ ] å®ç° `get_config_options()` (é…ç½®è§£æ)
- [ ] å•å…ƒæµ‹è¯•: é—´è·ä¿ç•™éªŒè¯
- [ ] å•å…ƒæµ‹è¯•: æµ®ç‚¹ç²¾åº¦è¾¹ç•Œcase

### Phase 2: AEPsyché›†æˆ (Day 1-2)

- [ ] ä¿®æ”¹ `aepsych/transforms/parameters.py`
- [ ] æ›´æ–° `aepsych/config.py` par_typeéªŒè¯
- [ ] æµ‹è¯•Transformå¾€è¿”ä¸€è‡´æ€§
- [ ] æµ‹è¯•boundsè½¬æ¢

### Phase 3: CustomPoolBasedGeneratoré›†æˆ (Day 2)

- [ ] ä¿®æ”¹ `_generate_pool_from_config()` ä½¿ç”¨è§„èŒƒåŒ–å€¼
- [ ] éªŒè¯poolåŒ…å«æ­£ç¡®çš„è§„èŒƒåŒ–å€¼
- [ ] æµ‹è¯•poolå»é‡é€»è¾‘

### Phase 4: LocalSampleré›†æˆ (Day 2-3)

- [ ] å®ç° `_perturb_ordinal()` (è§„èŒƒåŒ–å€¼ç©ºé—´æ‰°åŠ¨)
- [ ] ä¿®æ”¹ `sample()` è·¯ç”±åˆ°ordinalæ‰°åŠ¨
- [ ] æµ‹è¯•æœ€è¿‘é‚»çº¦æŸé€»è¾‘
- [ ] æµ‹è¯•æ··åˆæ‰°åŠ¨ç­–ç•¥

### Phase 5: æµ‹è¯•ä¸éªŒè¯ (Day 3)

- [ ] ç«¯åˆ°ç«¯æµ‹è¯•: éç­‰å·®å‚æ•°çš„é—´è·ä¿ç•™
- [ ] ANOVAï¿½ï¿½è¯: æ•ˆåº”ä¼°è®¡æ­£ç¡®æ€§
- [ ] æ€§èƒ½æµ‹è¯•: vs categorical baseline
- [ ] æ–‡æ¡£: ç©ºé—´çº¦å®šè¯´æ˜

---

**çŠ¶æ€**: è®¾è®¡å®Œæˆï¼Œå¾…å®æ–½
**æ¨è**: ç«‹å³é‡‡ç”¨æ­¤æ–¹æ¡ˆæ›¿ä»£åŸrankç©ºé—´æ–¹æ¡ˆ
