#!/usr/bin/env python3
"""
Phase 1 Step3: Base GP æ„å»ºä¸è®¾è®¡ç©ºé—´æ‰«æ (æ”¯æŒè¿ç»­å‹/åºæ•°å‹)

åŠŸèƒ½æ¦‚è¿°:
1. è¯»å– Phase1 æ•°æ®é›† (å«å› å­ä¸å“åº”)
2. æ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œæ•°æ®é¢„å¤„ç†:
   - è¿ç»­å‹ (continuous): è¢«è¯•å†… Z-score æ ‡å‡†åŒ–
   - åºæ•°å‹ (ordinal): è½¬æ¢ä¸º 0-indexed ç±»åˆ«æ ‡ç­¾ (ä¾‹å¦‚ Likert 1-5 -> 0-4)
3. è®­ç»ƒ GP æ¨¡å‹:
   - è¿ç»­å‹: Matern Î½=2.5 Kernel + ARD + GaussianLikelihood (ç²¾ç¡®æ¨æ–­)
   - åºæ•°å‹: AEPsych OrdinalGPModel (RBF + ARD + OrdinalLikelihood, å˜åˆ†æ¨æ–­)
4. æ‰«æç”¨æˆ·ç»™å®šçš„è®¾è®¡ç©ºé—´ CSV, è®¡ç®—é¢„æµ‹å‡å€¼/æ ‡å‡†å·®
5. é€‰å‡º: å…¨å±€æœ€é«˜ç‚¹ x_best_prior, å…¨å±€æœ€ä½ç‚¹ x_worst_prior, æœ€ä¸ç¡®å®šç‚¹ x_max_std
6. å¯¼å‡ºæ¨¡å‹ state_dict, é•¿åº¦å°ºåº¦, å…³é”®ç‚¹, è®¾è®¡ç©ºé—´æ‰«æç»“æœ, æŠ¥å‘Š

æ¨¡å‹ç±»å‹é€‰æ‹©:
  - model_type='continuous': é€‚ç”¨äºè¿ç»­å“åº”å˜é‡ (å¦‚çœŸå®å€¼æµ‹é‡)
  - model_type='ordinal': é€‚ç”¨äºåºæ•°å“åº”å˜é‡ (å¦‚ Likert é‡è¡¨ 1-5)

ä½¿ç”¨æ–¹å¼(é…ç½®è°ƒç”¨, æ¨èç»“åˆ quick_start.py):
  åœ¨ quick_start.py ä¸­è®¾ç½® MODE='step3' å¹¶å¡«å†™ STEP3_CONFIG

æ–‡ä»¶è¾“å‡º(é»˜è®¤ output_dir=base_gp_output):
  base_gp_state.pth               æ¨¡å‹ä¸likelihood state_dict (è¿ç»­å‹) æˆ–å®Œæ•´æ¨¡å‹ (åºæ•°å‹)
  base_gp_lengthscales.json       é•¿åº¦å°ºåº¦ä¸æ•æ„Ÿåº¦æ’åº
  base_gp_subject_stats.json      è¢«è¯•æ ‡å‡†åŒ–ç»Ÿè®¡ (è¿ç»­å‹) / ç±»åˆ«æ˜ å°„ (åºæ•°å‹)
  base_gp_encodings.json          åˆ†ç±»å˜é‡ç¼–ç æ˜ å°„
  base_gp_key_points.json         ä¸‰ä¸ªå…³é”®ç‚¹åŠé¢„æµ‹å€¼
  design_space_scan.csv           è®¾è®¡ç©ºé—´é€ç‚¹é¢„æµ‹ (mean,std)
  base_gp_report.md               æŠ¥å‘Šæ‘˜è¦

ä¾èµ–: torch, gpytorch, botorch, aepsych (åœ¨å½“å‰ pixi ç¯å¢ƒä¸­å·²æ»¡è¶³)
"""

from __future__ import annotations

import json
import sys
from pathlib import Path
from typing import Dict, Any, Tuple, List, Literal, Union

import numpy as np
import pandas as pd

try:
    import torch
    from torch import Tensor
    import gpytorch
    from botorch.models import SingleTaskGP
    from botorch.optim.fit import fit_gpytorch_mll_torch
    from gpytorch.mlls import ExactMarginalLogLikelihood
    from aepsych.models import OrdinalGPModel
    from aepsych.likelihoods import OrdinalLikelihood
except Exception as e:  # pragma: no cover
    print(f"[é”™è¯¯] éœ€è¦å®‰è£… torch/gpytorch/botorch/aepsych: {e}")
    sys.exit(1)


# ============================================================================
# Utility Functions (å…±ç”¨)
# ============================================================================

def _encode_factor_df(
    df: pd.DataFrame,
) -> Tuple[pd.DataFrame, Dict[str, Dict[Any, int]]]:
    """å¯¹å› å­åˆ—è¿›è¡Œç¼–ç  (åˆ†ç±»å˜é‡ label encode, bool->int)ã€‚
    è¿”å›ç¼–ç å DataFrame ä¸ ç¼–ç å­—å…¸ã€‚"""
    encoded = df.copy()
    encodings: Dict[str, Dict[Any, int]] = {}
    for col in encoded.columns:
        if encoded[col].dtype == "object":
            unique_vals = sorted(encoded[col].dropna().unique())
            mapping = {v: i for i, v in enumerate(unique_vals)}
            encodings[col] = mapping
            encoded[col] = encoded[col].map(mapping)
        elif encoded[col].dtype == "bool":
            mapping = {False: 0, True: 1}
            encodings[col] = mapping
            encoded[col] = encoded[col].astype(int)
    return encoded, encodings


def _apply_encodings(
    df: pd.DataFrame, encodings: Dict[str, Dict[Any, int]]
) -> pd.DataFrame:
    """å°†å·²å­˜åœ¨çš„ç¼–ç æ˜ å°„åº”ç”¨åˆ°æ–°çš„ DataFrame (è®¾è®¡ç©ºé—´)."""
    df_new = df.copy()
    for col, mapping in encodings.items():
        if col not in df_new.columns:
            raise ValueError(f"è®¾è®¡ç©ºé—´ç¼ºå¤±å› å­åˆ—: {col}")
        if df_new[col].dtype == "object":
            unknown = set(df_new[col].dropna().unique()) - set(mapping.keys())
            if unknown:
                raise ValueError(f"è®¾è®¡ç©ºé—´åˆ— {col} å‡ºç°æœªçŸ¥ç±»åˆ«: {unknown}")
            df_new[col] = df_new[col].map(mapping)
        elif df_new[col].dtype == "bool":
            df_new[col] = df_new[col].astype(int)
    return df_new


# ============================================================================
# Continuous GP (åŸæœ‰å®ç°)
# ============================================================================

def _standardize_subject_wise(
    df: pd.DataFrame, subject_col: str, response_col: str
) -> Tuple[np.ndarray, Dict[str, Dict[str, float]]]:
    """è¢«è¯•å†… Z-score æ ‡å‡†åŒ– (ä»…ç”¨äºè¿ç»­å‹æ¨¡å‹)."""
    subject_stats: Dict[str, Dict[str, float]] = {}
    y = df[response_col].values.astype(float)
    subjects = df[subject_col].astype(str).values
    y_norm = np.zeros_like(y)
    global_std = float(np.std(y)) + 1e-6
    for subj in np.unique(subjects):
        mask = subjects == subj
        y_subj = y[mask]
        mean_subj = float(np.mean(y_subj))
        std_subj = float(np.std(y_subj))
        adj_std = std_subj if std_subj > 1e-8 else global_std
        y_norm[mask] = (y_subj - mean_subj) / (adj_std + 1e-12)
        subject_stats[subj] = {
            "mean": mean_subj,
            "std": std_subj,
            "adjusted_std_used": adj_std,
            "n": int(mask.sum()),
        }
    return y_norm.astype(float), subject_stats


class _MaternARDGP(gpytorch.models.ExactGP):
    """è‡ªå®šä¹‰ Matern 2.5 + ARD ç²¾ç¡® GP (è¿ç»­å‹)."""

    def __init__(
        self,
        train_x: Tensor,
        train_y: Tensor,
        likelihood: gpytorch.likelihoods.GaussianLikelihood,
    ):
        super().__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.MaternKernel(
                nu=2.5,
                ard_num_dims=train_x.shape[-1],
            )
        )

    def forward(self, x: Tensor):  # type: ignore
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)


def train_continuous_gp(
    train_x: np.ndarray,
    train_y: np.ndarray,
    max_iters: int = 300,
    lr: float = 0.05,
    use_cuda: bool = True,
) -> Tuple[_MaternARDGP, gpytorch.likelihoods.GaussianLikelihood, Dict[str, Any]]:
    """è®­ç»ƒè¿ç»­å‹ Matern2.5+ARD GP."""
    device = torch.device("cuda" if use_cuda and torch.cuda.is_available() else "cpu")
    X = torch.from_numpy(train_x).float().to(device)
    y = torch.from_numpy(train_y).float().to(device)
    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)
    model = _MaternARDGP(X, y, likelihood).to(device)
    model.train()
    likelihood.train()
    optimizer = torch.optim.Adam([{"params": model.parameters()}], lr=lr)
    mll = ExactMarginalLogLikelihood(likelihood, model)
    log_history: List[Dict[str, float]] = []

    for it in range(1, max_iters + 1):
        optimizer.zero_grad()
        output = model(X)
        loss = -mll(output, y)
        loss.backward()
        optimizer.step()
        if it % 25 == 0 or it == 1:
            lengthscales = (
                model.covar_module.base_kernel.lengthscale.detach()
                .cpu()
                .numpy()
                .ravel()
                .tolist()
            )
            log_history.append(
                {
                    "iter": it,
                    "loss": float(loss.item()),
                    "noise": float(model.likelihood.noise.item()),
                    "lengthscale_mean": float(np.mean(lengthscales)),
                }
            )
    model.eval()
    likelihood.eval()
    return model, likelihood, {"device": str(device), "history": log_history}


def scan_design_space_continuous(
    model: _MaternARDGP,
    likelihood: gpytorch.likelihoods.GaussianLikelihood,
    design_x: np.ndarray,
    batch_size: int = 2048,
) -> Tuple[np.ndarray, np.ndarray]:
    """æ‰¹é‡é¢„æµ‹è®¾è®¡ç©ºé—´ (è¿ç»­å‹ GP)."""
    device = next(model.parameters()).device
    means: List[np.ndarray] = []
    stds: List[np.ndarray] = []
    with torch.no_grad(), gpytorch.settings.fast_pred_var():
        for start in range(0, design_x.shape[0], batch_size):
            end = start + batch_size
            Xbatch = torch.from_numpy(design_x[start:end]).float().to(device)
            pred = likelihood(model(Xbatch))
            means.append(pred.mean.cpu().numpy())
            stds.append(pred.stddev.cpu().numpy())
    mean_all = np.concatenate(means, axis=0)
    std_all = np.concatenate(stds, axis=0)
    return mean_all, std_all


# ============================================================================
# Ordinal GP (æ–°å¢å®ç°)
# ============================================================================

def convert_to_ordinal_labels(
    y_raw: np.ndarray, min_level: int = 1
) -> Tuple[np.ndarray, Dict[str, Any]]:
    """å°† Likert é‡è¡¨å€¼è½¬æ¢ä¸º 0-indexed ç±»åˆ«æ ‡ç­¾.

    ä¾‹å¦‚: Likert 1-5 -> 0-4

    Args:
        y_raw: åŸå§‹å“åº”å€¼ (å¦‚ 1,2,3,4,5)
        min_level: æœ€å°çº§åˆ«å€¼ (é»˜è®¤ 1)

    Returns:
        y_ordinal: 0-indexed æ ‡ç­¾ (å¦‚ 0,1,2,3,4)
        mapping: æ˜ å°„ä¿¡æ¯å­—å…¸
    """
    unique_levels = np.sort(np.unique(y_raw))
    n_levels = len(unique_levels)

    # åˆ›å»ºæ˜ å°„: åŸå§‹å€¼ -> 0-indexed
    level_to_index = {level: i for i, level in enumerate(unique_levels)}
    y_ordinal = np.array([level_to_index[val] for val in y_raw])

    mapping = {
        "n_levels": n_levels,
        "unique_levels": unique_levels.tolist(),
        "level_to_index": {int(k): int(v) for k, v in level_to_index.items()},
        "index_to_level": {int(v): int(k) for k, v in level_to_index.items()},
        "min_level": int(unique_levels[0]),
        "max_level": int(unique_levels[-1]),
    }

    return y_ordinal, mapping


def train_ordinal_gp(
    train_x: np.ndarray,
    train_y_ordinal: np.ndarray,
    n_levels: int,
    inducing_size: int = 100,
    use_cuda: bool = True,
) -> Tuple[OrdinalGPModel, Dict[str, Any]]:
    """è®­ç»ƒåºæ•°å‹ GP (ä½¿ç”¨ AEPsych OrdinalGPModel).

    Args:
        train_x: è¾“å…¥ç‰¹å¾ (n_samples, n_features)
        train_y_ordinal: 0-indexed åºæ•°æ ‡ç­¾ (n_samples,)
        n_levels: åºæ•°çº§åˆ«æ•°é‡
        inducing_size: è¯±å¯¼ç‚¹æ•°é‡
        use_cuda: æ˜¯å¦ä½¿ç”¨ CUDA

    Returns:
        model: è®­ç»ƒå¥½çš„åºæ•° GP æ¨¡å‹
        train_info: è®­ç»ƒä¿¡æ¯å­—å…¸
    """
    device = torch.device("cuda" if use_cuda and torch.cuda.is_available() else "cpu")

    # ç¡®ä¿ dtype ä¸€è‡´æ€§ï¼šä½¿ç”¨ double (float64)
    X = torch.from_numpy(train_x.astype(np.float64)).double()
    y = torch.from_numpy(train_y_ordinal.astype(np.int64)).long()

    # åˆ›å»º OrdinalLikelihood
    likelihood = OrdinalLikelihood(n_levels=n_levels)

    # åˆ›å»ºæ¨¡å‹
    model = OrdinalGPModel(
        dim=train_x.shape[1],
        likelihood=likelihood,
        inducing_size=min(inducing_size, train_x.shape[0]),
    )

    # è®­ç»ƒ
    print(f"[INFO] è®­ç»ƒåºæ•° GP: {train_x.shape[0]} æ ·æœ¬, {train_x.shape[1]} ç»´, {n_levels} ä¸ªç±»åˆ«")
    model.fit(X, y)

    # æå–è®­ç»ƒä¿¡æ¯
    # è·å– lengthscales (å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„)
    try:
        lengthscales = (
            model.covar_module.base_kernel.lengthscale.detach().cpu().numpy().ravel()
        )
    except:
        try:
            # OrdinalGPModel çš„ covar_module å¯èƒ½ç›´æ¥å°±æ˜¯ RBF kernel
            lengthscales = (
                model.covar_module.lengthscale.detach().cpu().numpy().ravel()
            )
        except:
            lengthscales = np.array([])

    # è·å– cutpoints
    try:
        cutpoints = model.likelihood.cutpoints.detach().cpu().numpy()
    except:
        cutpoints = np.array([])

    train_info = {
        "device": str(device),
        "n_inducing": inducing_size,
        "lengthscales": lengthscales.tolist() if len(lengthscales) > 0 else [],
        "cutpoints": cutpoints.tolist() if len(cutpoints) > 0 else [],
    }

    return model, train_info


def scan_design_space_ordinal(
    model: OrdinalGPModel,
    design_x: np.ndarray,
    batch_size: int = 2048,
) -> Tuple[np.ndarray, np.ndarray]:
    """æ‰¹é‡é¢„æµ‹è®¾è®¡ç©ºé—´ (åºæ•°å‹ GP).

    è¿”å›æ½œåœ¨å‡½æ•°çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆè€Œéæ¦‚ç‡åˆ†å¸ƒï¼‰ã€‚
    """
    means: List[np.ndarray] = []
    stds: List[np.ndarray] = []

    with torch.no_grad():
        for start in range(0, design_x.shape[0], batch_size):
            end = start + batch_size
            # ç¡®ä¿ dtype ä¸€è‡´æ€§ï¼šä½¿ç”¨ double (float64)
            Xbatch = torch.from_numpy(design_x[start:end].astype(np.float64)).double()

            # ä½¿ç”¨ predict æ–¹æ³•è·å–æ½œåœ¨å‡½æ•°çš„å‡å€¼å’Œæ–¹å·®
            fmean, fvar = model.predict(Xbatch)
            fstd = torch.sqrt(fvar)

            means.append(fmean.cpu().numpy())
            stds.append(fstd.cpu().numpy())

    mean_all = np.concatenate(means, axis=0)
    std_all = np.concatenate(stds, axis=0)
    return mean_all, std_all


# ============================================================================
# Common: Key Points Selection & Report
# ============================================================================

def select_key_points(
    design_df_encoded: pd.DataFrame,
    means: np.ndarray,
    stds: np.ndarray,
    ensure_diversity: bool = True,
) -> Dict[str, Any]:
    """é€‰æ‹©ä¸‰ä¸ªå…³é”®ç‚¹."""
    idx_best = int(np.argmax(means))
    idx_worst = int(np.argmin(means))
    max_std = float(np.max(stds))
    center_point = design_df_encoded.median(numeric_only=True).to_dict()

    if max_std < 1e-6:
        idx_std = -1
        max_std_mean = None
    else:
        idx_std = int(np.argmax(stds))
        max_std_mean = float(means[idx_std])

        if ensure_diversity and idx_std in (idx_best, idx_worst):
            sorted_indices = np.argsort(-stds)
            for candidate_idx in sorted_indices:
                if candidate_idx not in (idx_best, idx_worst):
                    idx_std = int(candidate_idx)
                    max_std = float(stds[idx_std])
                    max_std_mean = float(means[idx_std])
                    break

    return {
        "x_best_prior_index": idx_best,
        "x_best_prior": design_df_encoded.iloc[idx_best].to_dict(),
        "best_mean": float(means[idx_best]),
        "best_std": float(stds[idx_best]),
        "x_worst_prior_index": idx_worst,
        "x_worst_prior": design_df_encoded.iloc[idx_worst].to_dict(),
        "worst_mean": float(means[idx_worst]),
        "worst_std": float(stds[idx_worst]),
        "x_max_std_index": idx_std,
        "x_max_std": (
            design_df_encoded.iloc[idx_std].to_dict() if idx_std >= 0 else center_point
        ),
        "max_std": max_std if idx_std >= 0 else max_std,
        "max_std_mean": max_std_mean,
        "used_center_point": idx_std == -1,
        "center_point": center_point,
        "ensure_diversity": ensure_diversity,
    }


def write_report(
    path: Path,
    factor_names: List[str],
    lengthscales: List[float],
    metadata: Dict[str, Any],
    key_points: Dict[str, Any],
    train_meta: Dict[str, Any],
    model_type: str,
):
    """ç”Ÿæˆ Markdown æŠ¥å‘Š."""
    with open(path, "w", encoding="utf-8") as f:
        f.write(f"# Base GP æŠ¥å‘Š (æ¨¡å‹ç±»å‹: {model_type.upper()})\n\n")

        f.write("## ğŸ“ æ¨¡å‹ç»“æ„\n")
        if model_type == "continuous":
            f.write("- **ç±»å‹**: è¿ç»­å‹ GP (Exact Inference)\n")
            f.write("- **Kernel**: Matern(Î½=2.5) + ARD + Scale\n")
            f.write("- **Likelihood**: GaussianLikelihood\n")
        else:  # ordinal
            f.write("- **ç±»å‹**: åºæ•°å‹ GP (Variational Inference)\n")
            f.write("- **Kernel**: RBF + ARD\n")
            f.write("- **Likelihood**: OrdinalLikelihood\n")
            if "n_levels" in metadata:
                f.write(f"- **åºæ•°çº§åˆ«**: {metadata['n_levels']} ä¸ªç±»åˆ«\n")
                if "unique_levels" in metadata:
                    f.write(f"- **åŸå§‹å€¼èŒƒå›´**: {metadata['unique_levels']}\n")

        f.write(f"- **è¾“å…¥ç»´åº¦**: {len(factor_names)}\n")
        f.write(f"- **è®¾å¤‡**: {train_meta.get('device')}\n")

        f.write("\n## ğŸ”§ è®­ç»ƒæ‘˜è¦\n")
        if model_type == "continuous":
            hist = train_meta.get("history", [])
            if hist:
                f.write("| Iter | Loss | Noise | Mean Lengthscale |\n")
                f.write("|------|------|-------|------------------|\n")
                for row in hist:
                    f.write(
                        f"| {row['iter']} | {row['loss']:.3f} | {row['noise']:.3e} | {row['lengthscale_mean']:.3f} |\n"
                    )
        else:  # ordinal
            f.write(f"- **è¯±å¯¼ç‚¹æ•°é‡**: {train_meta.get('n_inducing', 'N/A')}\n")
            if "cutpoints" in train_meta and train_meta["cutpoints"]:
                f.write(f"- **Cutpoints**: {train_meta['cutpoints']}\n")

        f.write("\n## ğŸ›ï¸ é•¿åº¦å°ºåº¦ (Sensitivity)\n")
        if lengthscales:
            ranked = sorted(zip(factor_names, lengthscales), key=lambda x: x[1])
            f.write("| Rank | Factor | Lengthscale | Interpretation |\n")
            f.write("|------|--------|------------:|---------------|\n")
            for rank, (name, ls) in enumerate(ranked, 1):
                interp = (
                    "é«˜æ•æ„Ÿ (å˜åŒ–å°å³å½±å“å¤§)"
                    if rank <= max(1, len(ranked) // 3)
                    else ("ä¸­ç­‰" if rank <= 2 * len(ranked) // 3 else "ä½æ•æ„Ÿ")
                )
                f.write(f"| {rank} | {name} | {ls:.4f} | {interp} |\n")
        else:
            f.write("*é•¿åº¦å°ºåº¦ä¿¡æ¯ä¸å¯ç”¨*\n")

        f.write("\n## ğŸ“ å…³é”®ç‚¹ (è®¾è®¡ç©ºé—´)\n")
        for i, (label, key_prefix) in enumerate([
            ("Best Prior", "best"),
            ("Worst Prior", "worst"),
            ("Max Uncertainty", "uncertain"),
        ], 1):
            if key_prefix == "uncertain":
                coords = key_points["x_max_std"]
                mean_val = key_points.get("max_std_mean", 0.0)
                std_val = key_points["max_std"]
                is_center = key_points.get("used_center_point", False)
            else:
                coords = key_points[f"x_{key_prefix}_prior"]
                mean_val = key_points[f"{key_prefix}_mean"]
                std_val = key_points[f"{key_prefix}_std"]
                is_center = False

            coord_list = [coords[f] for f in factor_names]
            f.write(f"\n### {i} Sample {i} ({label})\n")
            if is_center:
                f.write("Note: Using center point (low variance)\n")
            f.write(f"- **Score**: Mean={mean_val:.3f}, Std={std_val:.3f}\n")
            f.write(f"- **Coordinates**: {coord_list}\n")

        f.write("\n*è‡ªåŠ¨ç”Ÿæˆ*\n")


# ============================================================================
# Main Processing Function
# ============================================================================

def process_step3(
    data_csv_path: str,
    design_space_csv: str,
    subject_col: str,
    response_col: str,
    output_dir: str,
    model_type: Literal["continuous", "ordinal"] = "continuous",
    max_iters: int = 300,
    lr: float = 0.05,
    use_cuda: bool = True,
    ensure_diversity: bool = True,
    inducing_size: int = 100,
    ordinal_min_level: int = 1,
) -> Dict[str, Any]:
    """æ ¸å¿ƒæµç¨‹: æ”¯æŒè¿ç»­å‹/åºæ•°å‹ GP.

    Args:
        data_csv_path: Phase1 æ•°æ® CSV è·¯å¾„
        design_space_csv: è®¾è®¡ç©ºé—´ CSV è·¯å¾„
        subject_col: è¢«è¯•åˆ—å
        response_col: å“åº”åˆ—å
        output_dir: è¾“å‡ºç›®å½•
        model_type: æ¨¡å‹ç±»å‹ ('continuous' | 'ordinal')
        max_iters: è®­ç»ƒè¿­ä»£æ•° (ä»…è¿ç»­å‹)
        lr: å­¦ä¹ ç‡ (ä»…è¿ç»­å‹)
        use_cuda: æ˜¯å¦ä½¿ç”¨ CUDA
        ensure_diversity: ç¡®ä¿å…³é”®ç‚¹å¤šæ ·æ€§
        inducing_size: è¯±å¯¼ç‚¹æ•°é‡ (ä»…åºæ•°å‹)
        ordinal_min_level: åºæ•°æœ€å°çº§åˆ«å€¼ (å¦‚ Likert 1-5 åˆ™ä¸º 1)
    """
    data_path = Path(data_csv_path)
    design_path = Path(design_space_csv)
    if not data_path.exists():
        raise FileNotFoundError(f"Phase1 æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_csv_path}")
    if not design_path.exists():
        raise FileNotFoundError(f"è®¾è®¡ç©ºé—´æ–‡ä»¶ä¸å­˜åœ¨: {design_space_csv}")

    df_phase1 = pd.read_csv(data_path)
    if subject_col not in df_phase1.columns or response_col not in df_phase1.columns:
        raise ValueError("Phase1 æ•°æ®ç¼ºå°‘å¿…è¦åˆ—")

    factor_cols = [c for c in df_phase1.columns if c not in (subject_col, response_col)]
    factor_df = df_phase1[factor_cols]
    encoded_factors, encodings = _encode_factor_df(factor_df)
    X_train = encoded_factors.values.astype(float)

    print(f"\n{'='*80}")
    print(f"è®­ç»ƒæ¨¡å‹ç±»å‹: {model_type.upper()}")
    print(f"{'='*80}")

    # ========== æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„è®­ç»ƒè·¯å¾„ ==========
    if model_type == "continuous":
        # è¿ç»­å‹: Z-score æ ‡å‡†åŒ–
        y_norm, subject_stats = _standardize_subject_wise(
            df_phase1[[subject_col, response_col]], subject_col, response_col
        )
        metadata = {"subject_stats": subject_stats}

        # è®­ç»ƒè¿ç»­å‹ GP
        model, likelihood, train_meta = train_continuous_gp(
            X_train, y_norm, max_iters=max_iters, lr=lr, use_cuda=use_cuda
        )

        lengthscales = (
            model.covar_module.base_kernel.lengthscale.detach()
            .cpu()
            .numpy()
            .ravel()
            .tolist()
        )

        # æ‰«æè®¾è®¡ç©ºé—´
        design_df_raw = pd.read_csv(design_path)
        missing_cols = set(factor_cols) - set(design_df_raw.columns)
        if missing_cols:
            raise ValueError(f"è®¾è®¡ç©ºé—´ç¼ºå°‘å› å­åˆ—: {missing_cols}")
        design_df_aligned = design_df_raw[factor_cols]
        design_df_encoded = _apply_encodings(design_df_aligned, encodings)

        means, stds = scan_design_space_continuous(
            model, likelihood, design_df_encoded.values.astype(float)
        )

        # ä¿å­˜æ¨¡å‹
        out_dir = Path(output_dir)
        out_dir.mkdir(parents=True, exist_ok=True)
        torch.save(
            {"model": model.state_dict(), "likelihood": likelihood.state_dict()},
            out_dir / "base_gp_state.pth",
        )

    else:  # ordinal
        # åºæ•°å‹: è½¬æ¢ä¸º 0-indexed
        y_raw = df_phase1[response_col].values
        y_ordinal, ordinal_mapping = convert_to_ordinal_labels(y_raw, ordinal_min_level)
        metadata = {"ordinal_mapping": ordinal_mapping, "n_levels": ordinal_mapping["n_levels"]}

        # è®­ç»ƒåºæ•°å‹ GP
        model, train_meta = train_ordinal_gp(
            X_train,
            y_ordinal,
            n_levels=ordinal_mapping["n_levels"],
            inducing_size=inducing_size,
            use_cuda=use_cuda,
        )

        lengthscales = train_meta.get("lengthscales", [])

        # æ‰«æè®¾è®¡ç©ºé—´
        design_df_raw = pd.read_csv(design_path)
        missing_cols = set(factor_cols) - set(design_df_raw.columns)
        if missing_cols:
            raise ValueError(f"è®¾è®¡ç©ºé—´ç¼ºå°‘å› å­åˆ—: {missing_cols}")
        design_df_aligned = design_df_raw[factor_cols]
        design_df_encoded = _apply_encodings(design_df_aligned, encodings)

        means, stds = scan_design_space_ordinal(
            model, design_df_encoded.values.astype(float)
        )

        # ä¿å­˜æ¨¡å‹ (åºæ•°å‹ä¿å­˜æ•´ä¸ªæ¨¡å‹)
        out_dir = Path(output_dir)
        out_dir.mkdir(parents=True, exist_ok=True)
        torch.save(model, out_dir / "base_gp_state.pth")
        metadata["unique_levels"] = ordinal_mapping["unique_levels"]

    # ========== é€šç”¨åå¤„ç† ==========
    key_points = select_key_points(
        design_df_encoded, means, stds, ensure_diversity=ensure_diversity
    )

    # ä¿å­˜å…¶ä»–è¾“å‡º
    (out_dir / "base_gp_lengthscales.json").write_text(
        json.dumps(
            {"factor_names": factor_cols, "lengthscales": lengthscales},
            indent=2,
            ensure_ascii=False,
        ),
        encoding="utf-8",
    )
    (out_dir / "base_gp_subject_stats.json").write_text(
        json.dumps(metadata, indent=2, ensure_ascii=False), encoding="utf-8"
    )
    (out_dir / "base_gp_encodings.json").write_text(
        json.dumps(encodings, indent=2, ensure_ascii=False), encoding="utf-8"
    )
    (out_dir / "base_gp_key_points.json").write_text(
        json.dumps(key_points, indent=2, ensure_ascii=False), encoding="utf-8"
    )

    scan_df = design_df_encoded.copy()
    scan_df["pred_mean"] = means
    scan_df["pred_std"] = stds
    scan_df.to_csv(out_dir / "design_space_scan.csv", index=False)

    write_report(
        out_dir / "base_gp_report.md",
        factor_cols,
        lengthscales,
        metadata,
        key_points,
        train_meta,
        model_type,
    )

    print(f"\n[OK] Output saved to: {out_dir}")
    return {
        "output_dir": str(out_dir),
        "model_type": model_type,
        "lengthscales": lengthscales,
        "key_points": key_points,
        "n_design_points": int(design_df_encoded.shape[0]),
    }


def main():  # pragma: no cover
    """äº¤äº’å¼ä¸»å…¥å£ (ç¤ºä¾‹)."""
    print("="*80)
    print("Phase 1 Step3: Base GP æ„å»ºä¸æ‰«æ (v2 - æ”¯æŒåºæ•°/è¿ç»­)")
    print("="*80)
    data_csv = input("Phase1 æ•°æ®CSVè·¯å¾„: ").strip() or "warmup_data.csv"
    design_csv = input("è®¾è®¡ç©ºé—´CSVè·¯å¾„: ").strip() or "design_space.csv"
    subject_col = input("è¢«è¯•åˆ—å [subject_id]: ").strip() or "subject_id"
    response_col = input("å“åº”åˆ—å [response]: ").strip() or "response"
    model_type = input("æ¨¡å‹ç±»å‹ (continuous/ordinal) [continuous]: ").strip() or "continuous"
    output_dir = input("è¾“å‡ºç›®å½• [base_gp_output]: ").strip() or "base_gp_output"

    if model_type not in ("continuous", "ordinal"):
        print(f"[é”™è¯¯] æ¨¡å‹ç±»å‹å¿…é¡»æ˜¯ 'continuous' æˆ– 'ordinal', å¾—åˆ°: {model_type}")
        sys.exit(1)

    process_step3(
        data_csv_path=data_csv,
        design_space_csv=design_csv,
        subject_col=subject_col,
        response_col=response_col,
        output_dir=output_dir,
        model_type=model_type,  # type: ignore
    )


if __name__ == "__main__":  # pragma: no cover
    main()
